[{"node type": "expr_stmt", "line number": "(52, 53)", "node content": "<ExprStmt: # pylint: disable=protected-access _store_sparse = sparse_ops._add_sparse_to_tensors_map@52,0>", "related code": "_store_sparse = sparse_ops._add_sparse_to_tensors_map\n"}, {"node type": "expr_stmt", "line number": "(53, 63)", "node content": "<ExprStmt: _store_many_sparse = sparse_ops._add_many_sparse_to_tensors_map@53,0>", "related code": "_store_many_sparse = sparse_ops._add_many_sparse_to_tensors_map\n"}, {"node type": "expr_stmt", "line number": "(54, 63)", "node content": "<ExprStmt: _restore_sparse = sparse_ops._take_many_sparse_from_tensors_map@54,0>", "related code": "_restore_sparse = sparse_ops._take_many_sparse_from_tensors_map\n"}, {"node type": "decorators", "line number": "(62, 0)", "node content": "PythonNode(decorators, [<Decorator: # pylint: enable=protected-access   @tf_export(     \"io.match_filenames_once\",     v1=[\"io.match_filenames_once\", \"train.match_filenames_once\"])@58,0>, <Decorator: @deprecation.deprecated_endpoints(\"train.match_filenames_once\")@61,0>])", "related code": "def match_filenames_once(pattern, name=None):\n"}, {"node type": "decorators", "line number": "(85, 0)", "node content": "PythonNode(decorators, [<Decorator: @tf_export(v1=[\"train.limit_epochs\"])@81,0>, <Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\")@82,0>])", "related code": "def limit_epochs(tensor, num_epochs=None, name=None):\n"}, {"node type": "decorators", "line number": "(123, 0)", "node content": "PythonNode(decorators, [<Decorator: @tf_export(v1=[\"train.input_producer\"])@117,0>, <Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.from_tensor_slices(input_tensor).shuffle\"     \"(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If \"     \"`shuffle=False`, omit the `.shuffle(...)`.\")@118,0>])", "related code": "def input_producer(input_tensor,\n"}, {"node type": "decorators", "line number": "(211, 0)", "node content": "PythonNode(decorators, [<Decorator: @tf_export(v1=[\"train.string_input_producer\"])@205,0>, <Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.from_tensor_slices(string_tensor).shuffle\"     \"(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If \"     \"`shuffle=False`, omit the `.shuffle(...)`.\")@206,0>])", "related code": "def string_input_producer(string_tensor,\n"}, {"node type": "decorators", "line number": "(285, 0)", "node content": "PythonNode(decorators, [<Decorator: @tf_export(v1=[\"train.range_input_producer\"])@280,0>, <Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If \"     \"`shuffle=False`, omit the `.shuffle(...)`.\")@281,0>])", "related code": "def range_input_producer(limit, num_epochs=None, shuffle=True, seed=None,\n"}, {"node type": "decorators", "line number": "(328, 0)", "node content": "PythonNode(decorators, [<Decorator: @tf_export(v1=[\"train.slice_input_producer\"])@322,0>, <Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle\"     \"(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If \"     \"`shuffle=False`, omit the `.shuffle(...)`.\")@323,0>])", "related code": "def slice_input_producer(tensor_list, num_epochs=None, shuffle=True, seed=None,\n"}, {"node type": "decorators", "line number": "(929, 0)", "node content": "PythonNode(decorators, [<Decorator: # Batching functions ----------------------------------------------------------   @tf_export(v1=[\"train.batch\"])@924,0>, <Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if \"     \"`dynamic_pad=True`).\")@925,0>])", "related code": "def batch(tensors, batch_size, num_threads=1, capacity=32,\n"}, {"node type": "decorators", "line number": "(1028, 0)", "node content": "PythonNode(decorators, [<Decorator: @tf_export(v1=[\"train.maybe_batch\"])@1023,0>, <Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.filter(...).batch(batch_size)` (or `padded_batch(...)`\"     \" if `dynamic_pad=True`).\")@1024,0>])", "related code": "def maybe_batch(tensors, keep_input, batch_size, num_threads=1, capacity=32,\n"}, {"node type": "decorators", "line number": "(1085, 0)", "node content": "PythonNode(decorators, [<Decorator: @tf_export(v1=[\"train.batch_join\"])@1080,0>, <Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.interleave(...).batch(batch_size)` (or \"     \"`padded_batch(...)` if `dynamic_pad=True`).\")@1081,0>])", "related code": "def batch_join(tensors_list, batch_size, capacity=32, enqueue_many=False,\n"}, {"node type": "decorators", "line number": "(1195, 0)", "node content": "PythonNode(decorators, [<Decorator: @tf_export(v1=[\"train.maybe_batch_join\"])@1190,0>, <Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.interleave(...).filter(...).batch(batch_size)` (or \"     \"`padded_batch(...)` if `dynamic_pad=True`).\")@1191,0>])", "related code": "def maybe_batch_join(tensors_list, keep_input, batch_size, capacity=32,\n"}, {"node type": "decorators", "line number": "(1251, 0)", "node content": "PythonNode(decorators, [<Decorator: @tf_export(v1=[\"train.shuffle_batch\"])@1247,0>, <Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\")@1248,0>])", "related code": "def shuffle_batch(tensors, batch_size, capacity, min_after_dequeue,\n"}, {"node type": "decorators", "line number": "(1355, 0)", "node content": "PythonNode(decorators, [<Decorator: @tf_export(v1=[\"train.maybe_shuffle_batch\"])@1350,0>, <Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.filter(...).shuffle(min_after_dequeue).batch(batch_size)`\"     \".\")@1351,0>])", "related code": "def maybe_shuffle_batch(tensors, batch_size, capacity, min_after_dequeue,\n"}, {"node type": "decorators", "line number": "(1419, 0)", "node content": "PythonNode(decorators, [<Decorator: @tf_export(v1=[\"train.shuffle_batch_join\"])@1414,0>, <Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.interleave(...).shuffle(min_after_dequeue).batch\"     \"(batch_size)`.\")@1415,0>])", "related code": "def shuffle_batch_join(tensors_list, batch_size, capacity,\n"}, {"node type": "decorators", "line number": "(1517, 0)", "node content": "PythonNode(decorators, [<Decorator: @tf_export(v1=[\"train.maybe_shuffle_batch_join\"])@1512,0>, <Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.interleave(...).filter(...).shuffle(min_after_dequeue)\"     \".batch(batch_size)`.\")@1513,0>])", "related code": "def maybe_shuffle_batch_join(tensors_list, batch_size, capacity,\n"}, {"node type": "decorator", "line number": "(61, 0)", "node content": "<Decorator: # pylint: enable=protected-access   @tf_export(     \"io.match_filenames_once\",     v1=[\"io.match_filenames_once\", \"train.match_filenames_once\"])@58,0>", "related code": "@deprecation.deprecated_endpoints(\"train.match_filenames_once\")\n"}, {"node type": "decorator", "line number": "(62, 0)", "node content": "<Decorator: @deprecation.deprecated_endpoints(\"train.match_filenames_once\")@61,0>", "related code": "def match_filenames_once(pattern, name=None):\n"}, {"node type": "decorator", "line number": "(82, 0)", "node content": "<Decorator: @tf_export(v1=[\"train.limit_epochs\"])@81,0>", "related code": "@deprecation.deprecated(\n"}, {"node type": "decorator", "line number": "(85, 0)", "node content": "<Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\")@82,0>", "related code": "def limit_epochs(tensor, num_epochs=None, name=None):\n"}, {"node type": "decorator", "line number": "(118, 0)", "node content": "<Decorator: @tf_export(v1=[\"train.input_producer\"])@117,0>", "related code": "@deprecation.deprecated(\n"}, {"node type": "decorator", "line number": "(123, 0)", "node content": "<Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.from_tensor_slices(input_tensor).shuffle\"     \"(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If \"     \"`shuffle=False`, omit the `.shuffle(...)`.\")@118,0>", "related code": "def input_producer(input_tensor,\n"}, {"node type": "decorator", "line number": "(206, 0)", "node content": "<Decorator: @tf_export(v1=[\"train.string_input_producer\"])@205,0>", "related code": "@deprecation.deprecated(\n"}, {"node type": "decorator", "line number": "(211, 0)", "node content": "<Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.from_tensor_slices(string_tensor).shuffle\"     \"(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If \"     \"`shuffle=False`, omit the `.shuffle(...)`.\")@206,0>", "related code": "def string_input_producer(string_tensor,\n"}, {"node type": "decorator", "line number": "(281, 0)", "node content": "<Decorator: @tf_export(v1=[\"train.range_input_producer\"])@280,0>", "related code": "@deprecation.deprecated(\n"}, {"node type": "decorator", "line number": "(285, 0)", "node content": "<Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If \"     \"`shuffle=False`, omit the `.shuffle(...)`.\")@281,0>", "related code": "def range_input_producer(limit, num_epochs=None, shuffle=True, seed=None,\n"}, {"node type": "decorator", "line number": "(323, 0)", "node content": "<Decorator: @tf_export(v1=[\"train.slice_input_producer\"])@322,0>", "related code": "@deprecation.deprecated(\n"}, {"node type": "decorator", "line number": "(328, 0)", "node content": "<Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle\"     \"(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If \"     \"`shuffle=False`, omit the `.shuffle(...)`.\")@323,0>", "related code": "def slice_input_producer(tensor_list, num_epochs=None, shuffle=True, seed=None,\n"}, {"node type": "decorator", "line number": "(925, 0)", "node content": "<Decorator: # Batching functions ----------------------------------------------------------   @tf_export(v1=[\"train.batch\"])@924,0>", "related code": "@deprecation.deprecated(\n"}, {"node type": "decorator", "line number": "(929, 0)", "node content": "<Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if \"     \"`dynamic_pad=True`).\")@925,0>", "related code": "def batch(tensors, batch_size, num_threads=1, capacity=32,\n"}, {"node type": "decorator", "line number": "(1024, 0)", "node content": "<Decorator: @tf_export(v1=[\"train.maybe_batch\"])@1023,0>", "related code": "@deprecation.deprecated(\n"}, {"node type": "decorator", "line number": "(1028, 0)", "node content": "<Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.filter(...).batch(batch_size)` (or `padded_batch(...)`\"     \" if `dynamic_pad=True`).\")@1024,0>", "related code": "def maybe_batch(tensors, keep_input, batch_size, num_threads=1, capacity=32,\n"}, {"node type": "decorator", "line number": "(1081, 0)", "node content": "<Decorator: @tf_export(v1=[\"train.batch_join\"])@1080,0>", "related code": "@deprecation.deprecated(\n"}, {"node type": "decorator", "line number": "(1085, 0)", "node content": "<Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.interleave(...).batch(batch_size)` (or \"     \"`padded_batch(...)` if `dynamic_pad=True`).\")@1081,0>", "related code": "def batch_join(tensors_list, batch_size, capacity=32, enqueue_many=False,\n"}, {"node type": "decorator", "line number": "(1191, 0)", "node content": "<Decorator: @tf_export(v1=[\"train.maybe_batch_join\"])@1190,0>", "related code": "@deprecation.deprecated(\n"}, {"node type": "decorator", "line number": "(1195, 0)", "node content": "<Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.interleave(...).filter(...).batch(batch_size)` (or \"     \"`padded_batch(...)` if `dynamic_pad=True`).\")@1191,0>", "related code": "def maybe_batch_join(tensors_list, keep_input, batch_size, capacity=32,\n"}, {"node type": "decorator", "line number": "(1248, 0)", "node content": "<Decorator: @tf_export(v1=[\"train.shuffle_batch\"])@1247,0>", "related code": "@deprecation.deprecated(\n"}, {"node type": "decorator", "line number": "(1251, 0)", "node content": "<Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\")@1248,0>", "related code": "def shuffle_batch(tensors, batch_size, capacity, min_after_dequeue,\n"}, {"node type": "decorator", "line number": "(1351, 0)", "node content": "<Decorator: @tf_export(v1=[\"train.maybe_shuffle_batch\"])@1350,0>", "related code": "@deprecation.deprecated(\n"}, {"node type": "decorator", "line number": "(1355, 0)", "node content": "<Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.filter(...).shuffle(min_after_dequeue).batch(batch_size)`\"     \".\")@1351,0>", "related code": "def maybe_shuffle_batch(tensors, batch_size, capacity, min_after_dequeue,\n"}, {"node type": "decorator", "line number": "(1415, 0)", "node content": "<Decorator: @tf_export(v1=[\"train.shuffle_batch_join\"])@1414,0>", "related code": "@deprecation.deprecated(\n"}, {"node type": "decorator", "line number": "(1419, 0)", "node content": "<Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.interleave(...).shuffle(min_after_dequeue).batch\"     \"(batch_size)`.\")@1415,0>", "related code": "def shuffle_batch_join(tensors_list, batch_size, capacity,\n"}, {"node type": "decorator", "line number": "(1513, 0)", "node content": "<Decorator: @tf_export(v1=[\"train.maybe_shuffle_batch_join\"])@1512,0>", "related code": "@deprecation.deprecated(\n"}, {"node type": "decorator", "line number": "(1517, 0)", "node content": "<Decorator: @deprecation.deprecated(     None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"     \"`tf.data.Dataset.interleave(...).filter(...).shuffle(min_after_dequeue)\"     \".batch(batch_size)`.\")@1513,0>", "related code": "def maybe_shuffle_batch_join(tensors_list, batch_size, capacity,\n"}, {"node type": "string", "line number": "(61, 62)", "node content": "<String: \"train.match_filenames_once\">", "related code": "@deprecation.deprecated_endpoints(\"train.match_filenames_once\")\n"}, {"node type": "not_test", "line number": "(453, 21)", "node content": "PythonNode(not_test, [<Keyword: not>, <Name: tensors_list@453,9>])", "related code": "  if not tensors_list:\n"}, {"node type": "expr_stmt", "line number": "(509, 68)", "node content": "<ExprStmt: maybe_shared_map_ops = shared_map_ops or [None] * len(tensor_list)@509,2>", "related code": "  maybe_shared_map_ops = shared_map_ops or [None] * len(tensor_list)\n"}, {"node type": "expr_stmt", "line number": "(584, 25)", "node content": "<ExprStmt: stored_list_list = [s0]@584,2>", "related code": "  stored_list_list = [s0]\n"}, {"node type": "expr_stmt", "line number": "(602, 71)", "node content": "<ExprStmt: received_sequence = isinstance(stored_list, collections_abc.Sequence)@602,2>", "related code": "  received_sequence = isinstance(stored_list, collections_abc.Sequence)\n"}]
