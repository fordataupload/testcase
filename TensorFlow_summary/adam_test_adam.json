[{"node type": "decorator", "line number": "(32, 0)", "node content": "<Decorator: @tf_export(v1=[\"train.AdamOptimizer\"])@31,0>", "related code": "class AdamOptimizer(optimizer.Optimizer):\n"}, {"node type": "string", "line number": "(31, 36)", "node content": "<String: \"train.AdamOptimizer\">", "related code": "@tf_export(v1=[\"train.AdamOptimizer\"])\n"}, {"node type": "expr_stmt", "line number": "(98, 28)", "node content": "<ExprStmt: self._lr = learning_rate@98,4>", "related code": "    self._lr = learning_rate\n"}, {"node type": "expr_stmt", "line number": "(99, 23)", "node content": "<ExprStmt: self._beta1 = beta1@99,4>", "related code": "    self._beta1 = beta1\n"}, {"node type": "expr_stmt", "line number": "(100, 23)", "node content": "<ExprStmt: self._beta2 = beta2@100,4>", "related code": "    self._beta2 = beta2\n"}, {"node type": "expr_stmt", "line number": "(101, 27)", "node content": "<ExprStmt: self._epsilon = epsilon@101,4>", "related code": "    self._epsilon = epsilon\n"}, {"node type": "expr_stmt", "line number": "(104, 21)", "node content": "<ExprStmt: # Tensor versions of the constructor arguments, created in _prepare().     self._lr_t = None@104,4>", "related code": "    self._lr_t = None\n"}, {"node type": "expr_stmt", "line number": "(105, 24)", "node content": "<ExprStmt: self._beta1_t = None@105,4>", "related code": "    self._beta1_t = None\n"}, {"node type": "expr_stmt", "line number": "(106, 24)", "node content": "<ExprStmt: self._beta2_t = None@106,4>", "related code": "    self._beta2_t = None\n"}, {"node type": "expr_stmt", "line number": "(107, 26)", "node content": "<ExprStmt: self._epsilon_t = None@107,4>", "related code": "    self._epsilon_t = None\n"}, {"node type": "expr_stmt", "line number": "(123, 51)", "node content": "<ExprStmt: # Create the beta1 and beta2 accumulators on the same device as the first     # variable. Sort the var_list to make sure this device is consistent across     # workers (these need to go on the same PS, otherwise some updates are     # silently ignored).     first_var = min(var_list, key=lambda x: x.name)@123,4>", "related code": "    first_var = min(var_list, key=lambda x: x.name)\n"}, {"node type": "expr_stmt", "line number": "(135, 41)", "node content": "<ExprStmt: lr = self._call_if_callable(self._lr)@135,4>", "related code": "    lr = self._call_if_callable(self._lr)\n"}, {"node type": "expr_stmt", "line number": "(136, 47)", "node content": "<ExprStmt: beta1 = self._call_if_callable(self._beta1)@136,4>", "related code": "    beta1 = self._call_if_callable(self._beta1)\n"}, {"node type": "expr_stmt", "line number": "(137, 47)", "node content": "<ExprStmt: beta2 = self._call_if_callable(self._beta2)@137,4>", "related code": "    beta2 = self._call_if_callable(self._beta2)\n"}, {"node type": "expr_stmt", "line number": "(138, 51)", "node content": "<ExprStmt: epsilon = self._call_if_callable(self._epsilon)@138,4>", "related code": "    epsilon = self._call_if_callable(self._epsilon)\n"}, {"node type": "expr_stmt", "line number": "(140, 64)", "node content": "<ExprStmt: self._lr_t = ops.convert_to_tensor(lr, name=\"learning_rate\")@140,4>", "related code": "    self._lr_t = ops.convert_to_tensor(lr, name=\"learning_rate\")\n"}, {"node type": "operator", "line number": "(190, 30)", "node content": "<Operator: *>", "related code": "    m_scaled_g_values = grad * (1 - beta1_t)\n"}, {"node type": "operator", "line number": "(196, 39)", "node content": "<Operator: *>", "related code": "    v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n"}, {"node type": "operator", "line number": "(187, 16)", "node content": "<Operator: *>", "related code": "    lr = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n"}, {"node type": "operator", "line number": "(187, 49)", "node content": "<Operator: />", "related code": "    lr = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n"}, {"node type": "string", "line number": "(146, 30)", "node content": "<String: \"m\">", "related code": "    m = self.get_slot(var, \"m\")\n"}, {"node type": "string", "line number": "(147, 30)", "node content": "<String: \"v\">", "related code": "    v = self.get_slot(var, \"v\")\n"}, {"node type": "string", "line number": "(163, 30)", "node content": "<String: \"m\">", "related code": "    m = self.get_slot(var, \"m\")\n"}, {"node type": "string", "line number": "(164, 30)", "node content": "<String: \"v\">", "related code": "    v = self.get_slot(var, \"v\")\n"}, {"node type": "string", "line number": "(189, 30)", "node content": "<String: \"m\">", "related code": "    m = self.get_slot(var, \"m\")\n"}, {"node type": "number", "line number": "(190, 33)", "node content": "<Number: 1>", "related code": "    m_scaled_g_values = grad * (1 - beta1_t)\n"}, {"node type": "operator", "line number": "(190, 35)", "node content": "<Operator: ->", "related code": "    m_scaled_g_values = grad * (1 - beta1_t)\n"}, {"node type": "string", "line number": "(195, 30)", "node content": "<String: \"v\">", "related code": "    v = self.get_slot(var, \"v\")\n"}, {"node type": "operator", "line number": "(196, 31)", "node content": "<Operator: *>", "related code": "    v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n"}, {"node type": "number", "line number": "(196, 42)", "node content": "<Number: 1>", "related code": "    v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n"}, {"node type": "operator", "line number": "(196, 44)", "node content": "<Operator: ->", "related code": "    v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n"}, {"node type": "lambdef", "line number": "(123, 50)", "node content": "<Lambda@(123, 34)>", "related code": "    first_var = min(var_list, key=lambda x: x.name)\n"}, {"node type": "string", "line number": "(131, 29)", "node content": "<String: \"m\">", "related code": "      self._zeros_slot(v, \"m\", self._name)\n"}, {"node type": "string", "line number": "(132, 29)", "node content": "<String: \"v\">", "related code": "      self._zeros_slot(v, \"v\", self._name)\n"}, {"node type": "string", "line number": "(140, 63)", "node content": "<String: \"learning_rate\">", "related code": "    self._lr_t = ops.convert_to_tensor(lr, name=\"learning_rate\")\n"}, {"node type": "string", "line number": "(141, 61)", "node content": "<String: \"beta1\">", "related code": "    self._beta1_t = ops.convert_to_tensor(beta1, name=\"beta1\")\n"}, {"node type": "string", "line number": "(142, 61)", "node content": "<String: \"beta2\">", "related code": "    self._beta2_t = ops.convert_to_tensor(beta2, name=\"beta2\")\n"}, {"node type": "string", "line number": "(143, 67)", "node content": "<String: \"epsilon\">", "related code": "    self._epsilon_t = ops.convert_to_tensor(epsilon, name=\"epsilon\")\n"}, {"node type": "number", "line number": "(187, 52)", "node content": "<Number: 1>", "related code": "    lr = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n"}, {"node type": "operator", "line number": "(187, 54)", "node content": "<Operator: ->", "related code": "    lr = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n"}, {"node type": "operator", "line number": "(191, 33)", "node content": "<Operator: *>", "related code": "    m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\n"}, {"node type": "operator", "line number": "(197, 33)", "node content": "<Operator: *>", "related code": "    v_t = state_ops.assign(v, v * beta2_t, use_locking=self._use_locking)\n"}, {"node type": "number", "line number": "(187, 32)", "node content": "<Number: 1>", "related code": "    lr = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n"}, {"node type": "operator", "line number": "(187, 34)", "node content": "<Operator: ->", "related code": "    lr = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n"}, {"node type": "string", "line number": "(115, 55)", "node content": "<String: \"beta1_power\">", "related code": "      return (self._get_non_slot_variable(\"beta1_power\", graph=graph),\n"}]
