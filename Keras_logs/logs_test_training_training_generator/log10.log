2020-10-04 14:37:40.434670: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
============================= test session starts =============================
platform win32 -- Python 3.6.12, pytest-6.0.2, py-1.9.0, pluggy-0.13.1
rootdir: C:\Users\mutation\Desktop\testcase\tests\keras\engine
plugins: flaky-3.7.0
collected 34 items

test_training.py ....F..s..........................                      [100%]

================================== FAILURES ===================================
_____________________________ test_fit_generator ______________________________

    @flaky(rerun_filter=lambda err, *args: issubclass(err[0], AssertionError))
    def test_fit_generator():
        model = get_model(num_outputs=2)
        optimizer = 'rmsprop'
        loss = 'mse'
        loss_weights = [1., 0.5]
    
        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,
                      sample_weight_mode=None)
        tracker_cb = TrackerCallback()
        val_seq = RandomSequence(4)
        out = model.fit_generator(generator=RandomSequence(3),
                                  steps_per_epoch=3,
                                  epochs=5,
                                  initial_epoch=0,
                                  validation_data=val_seq,
                                  validation_steps=3,
                                  max_queue_size=1,
                                  callbacks=[tracker_cb])
        assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
        assert tracker_cb.trained_batches == list(range(3)) * 5
        assert len(val_seq.logs) <= 4 * 5
    
        tracker_cb = TrackerCallback()
        val_seq = RandomSequence(4)
        out = model.fit(RandomSequence(3),
                        steps_per_epoch=3,
                        epochs=5,
                        initial_epoch=0,
                        validation_data=val_seq,
                        validation_steps=3,
                        max_queue_size=1,
                        callbacks=[tracker_cb])
        assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
        assert tracker_cb.trained_batches == list(range(3)) * 5
        assert len(val_seq.logs) <= 4 * 5
    
        # steps_per_epoch will be equal to len of sequence if it's unspecified
        tracker_cb = TrackerCallback()
        val_seq = RandomSequence(4)
        out = model.fit_generator(generator=RandomSequence(3),
                                  epochs=5,
                                  initial_epoch=0,
                                  validation_data=val_seq,
                                  callbacks=[tracker_cb],
                                  max_queue_size=1)
        assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
        assert tracker_cb.trained_batches == list(range(12)) * 5
        assert 12 * 5 <= len(val_seq.logs) <= (12 * 5) + 2  # the queue may be full.
    
        tracker_cb = TrackerCallback()
        val_seq = RandomSequence(4)
        out = model.fit(RandomSequence(3),
                        epochs=5,
                        initial_epoch=0,
                        validation_data=val_seq,
                        callbacks=[tracker_cb],
                        max_queue_size=1)
        assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
        assert tracker_cb.trained_batches == list(range(12)) * 5
        assert 12 * 5 <= len(val_seq.logs) <= (12 * 5) + 2  # the queue may be full.
    
        # test for workers = 0
        tracker_cb = TrackerCallback()
        val_seq = RandomSequence(4)
        out = model.fit_generator(generator=RandomSequence(3),
                                  epochs=5,
                                  validation_data=val_seq,
                                  callbacks=[tracker_cb],
>                                 workers=0)

test_training.py:541: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\legacy\interfaces.py:91: in wrapper
    return func(*args, **kwargs)
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training.py:1732: in fit_generator
    initial_epoch=initial_epoch)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = <keras.engine.training.Model object at 0x000001D21FE91358>
generator = <test_training.RandomSequence object at 0x000001D220405630>
steps_per_epoch = 12, epochs = 5, verbose = 1
callbacks = <keras.callbacks.callbacks.CallbackList object at 0x000001D220463668>
validation_data = <test_training.RandomSequence object at 0x000001D22045E518>
validation_steps = 12, validation_freq = 1, class_weight = None
max_queue_size = 10, workers = 0, use_multiprocessing = False, shuffle = True
initial_epoch = 0

    def fit_generator(model,
                      generator,
                      steps_per_epoch=None,
                      epochs=1,
                      verbose=1,
                      callbacks=None,
                      validation_data=None,
                      validation_steps=None,
                      validation_freq=1,
                      class_weight=None,
                      max_queue_size=10,
                      workers=1,
                      use_multiprocessing=False,
                      shuffle=True,
                      initial_epoch=0):
        """See docstring for `Model.fit_generator`."""
        epoch = initial_epoch
    
        do_validation = bool(validation_data)
        model._make_train_function()
        if do_validation:
            model._make_test_function()
    
        use_sequence_api = is_sequence(generator)
        if not use_sequence_api and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the `keras.utils.Sequence'
                            ' class.'))
    
        # if generator is instance of Sequence and steps_per_epoch are not provided -
        # recompute steps_per_epoch after each epoch
        recompute_steps_per_epoch = use_sequence_api and steps_per_epoch is None
    
        if steps_per_epoch is None:
            if use_sequence_api:
                steps_per_epoch = len(generator)
            else:
                raise ValueError('`steps_per_epoch=None` is only valid for a'
                                 ' generator based on the '
                                 '`keras.utils.Sequence`'
                                 ' class. Please specify `steps_per_epoch` '
                                 'or use the `keras.utils.Sequence` class.')
    
        # python 2 has 'next', 3 has '__next__'
        # avoid any explicit version checks
        val_use_sequence_api = is_sequence(validation_data)
        val_gen = (hasattr(validation_data, 'next') or
                   hasattr(validation_data, '__next__') or
                   val_use_sequence_api)
        if (val_gen and not val_use_sequence_api and
                not validation_steps):
            raise ValueError('`validation_steps=None` is only valid for a'
                             ' generator based on the `keras.utils.Sequence`'
                             ' class. Please specify `validation_steps` or use'
                             ' the `keras.utils.Sequence` class.')
    
        # Prepare display labels.
        out_labels = model.metrics_names
        callback_metrics = out_labels + ['val_' + n for n in out_labels]
    
        # prepare callbacks
        model.history = cbks.History()
        _callbacks = [cbks.BaseLogger(
            stateful_metrics=model.metrics_names[1:])]
        if verbose:
            _callbacks.append(
                cbks.ProgbarLogger(
                    count_mode='steps',
                    stateful_metrics=model.metrics_names[1:]))
        _callbacks += (callbacks or []) + [model.history]
        callbacks = cbks.CallbackList(_callbacks)
    
        # it's possible to callback a different model than self:
        callback_model = model._get_callback_model()
    
        callbacks.set_model(callback_model)
        callbacks.set_params({
            'epochs': epochs,
            'steps': steps_per_epoch,
            'verbose': verbose,
            'do_validation': do_validation,
            'metrics': callback_metrics,
        })
        callbacks._call_begin_hook('train')
    
        enqueuer = ""
        val_enqueuer = None
    
        try:
            if do_validation:
                if val_gen and workers > 0:
                    # Create an Enqueuer that can be reused
                    val_data = validation_data
                    if is_sequence(val_data):
                        val_enqueuer = OrderedEnqueuer(
                            val_data,
                            use_multiprocessing=use_multiprocessing)
                        validation_steps = validation_steps or len(val_data)
                    else:
                        val_enqueuer = GeneratorEnqueuer(
                            val_data,
                            use_multiprocessing=use_multiprocessing)
                    val_enqueuer.start(workers=workers,
                                       max_queue_size=max_queue_size)
                    val_enqueuer_gen = val_enqueuer.get()
                elif val_gen:
                    val_data = validation_data
                    if is_sequence(val_data):
                        val_enqueuer_gen = iter_sequence_infinite(val_data)
                        validation_steps = validation_steps or len(val_data)
                    else:
                        val_enqueuer_gen = val_data
                else:
                    # Prepare data for validation
                    if len(validation_data) == 2:
                        val_x, val_y = validation_data
                        val_sample_weight = None
                    elif len(validation_data) == 3:
                        val_x, val_y, val_sample_weight = validation_data
                    else:
                        raise ValueError('`validation_data` should be a tuple '
                                         '`(val_x, val_y, val_sample_weight)` '
                                         'or `(val_x, val_y)`. Found: ' +
                                         str(validation_data))
                    val_x, val_y, val_sample_weights = model._standardize_user_data(
                        val_x, val_y, val_sample_weight)
                    val_data = val_x + val_y + val_sample_weights
                    if model.uses_learning_phase and not isinstance(K.learning_phase(),
                                                                    int):
                        val_data += [0.]
                    for cbk in callbacks:
                        cbk.validation_data = val_data
    
            if workers > 0:
                if use_sequence_api:
                    enqueuer = OrderedEnqueuer(
                        generator,
                        use_multiprocessing=use_multiprocessing,
                        shuffle=shuffle)
                else:
                    enqueuer = GeneratorEnqueuer(
                        generator,
                        use_multiprocessing=use_multiprocessing)
                enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                output_generator = enqueuer.get()
            else:
                if use_sequence_api:
                    output_generator = iter_sequence_infinite(generator)
                else:
                    output_generator = generator
    
            callbacks.model.stop_training = False
            # Construct epoch logs.
            epoch_logs = {}
            while epoch < epochs:
                model.reset_metrics()
                callbacks.on_epoch_begin(epoch)
                steps_done = 0
                batch_index = 0
                while steps_done < steps_per_epoch:
                    generator_output = next(output_generator)
    
                    if not hasattr(generator_output, '__len__'):
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
    
                    if len(generator_output) == 2:
                        x, y = generator_output
                        sample_weight = None
                    elif len(generator_output) == 3:
                        x, y, sample_weight = generator_output
                    else:
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
                    if x is None or len(x) == 0:
                        # Handle data tensors support when no input given
                        # step-size = 1 for data tensors
                        batch_size = 1
                    elif isinstance(x, list):
                        batch_size = x[0].shape[0]
                    elif isinstance(x, dict):
                        batch_size = list(x.values())[0].shape[0]
                    else:
                        batch_size = x.shape[0]
                    # build batch logs
                    batch_logs = {'batch': batch_index, 'size': batch_size}
                    callbacks.on_batch_begin(batch_index, batch_logs)
    
                    outs = model.train_on_batch(x, y,
                                                sample_weight=sample_weight,
                                                class_weight=class_weight,
                                                reset_metrics=False)
    
                    outs = to_list(outs)
                    for l, o in zip(out_labels, outs):
                        batch_logs[l] = o
    
                    callbacks._call_batch_hook('train', 'end', batch_index, batch_logs)
    
                    batch_index += 1
                    steps_done += 1
    
                    # Epoch finished.
                    if (steps_done >= steps_per_epoch and
                            do_validation and
                            should_run_validation(validation_freq, epoch)):
                        # Note that `callbacks` here is an instance of
                        # `keras.callbacks.CallbackList`
                        if val_gen:
                            val_outs = model.evaluate_generator(
                                val_enqueuer_gen,
                                validation_steps,
                                callbacks=callbacks,
                                workers=0)
                        else:
                            # No need for try/except because
                            # data has already been validated.
                            val_outs = model.evaluate(
                                val_x, val_y,
                                batch_size=batch_size,
                                sample_weight=val_sample_weights,
                                callbacks=callbacks,
                                verbose=0)
                        val_outs = to_list(val_outs)
                        # Same labels assumed.
                        for l, o in zip(out_labels, val_outs):
                            epoch_logs['val_' + l] = o
    
                    if callbacks.model.stop_training:
                        break
    
                callbacks.on_epoch_end(epoch, epoch_logs)
                epoch += 1
                if callbacks.model.stop_training:
                    break
    
                if use_sequence_api and workers == 0:
                    generator.on_epoch_end()
    
                if recompute_steps_per_epoch:
                    if workers > 0:
                        enqueuer.join_end_of_epoch()
    
                    # recomute steps per epochs in case if Sequence changes it's length
                    steps_per_epoch = len(generator)
    
                    # update callbacks to make sure params are valid each epoch
                    callbacks.set_params({
                        'epochs': epochs,
                        'steps': steps_per_epoch,
                        'verbose': verbose,
                        'do_validation': do_validation,
                        'metrics': callback_metrics,
                    })
    
        finally:
            try:
                if enqueuer is not None:
>                   enqueuer.stop()
E                   AttributeError: 'str' object has no attribute 'stop'

C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_generator.py:287: AttributeError
---------------------------- Captured stdout call -----------------------------
Epoch 1/5

1/3 [=========>....................] - ETA: 0s - loss: 0.9288 - dense_1_loss: 0.6536 - dropout_loss: 0.5503
3/3 [==============================] - 0s 47ms/step - loss: 0.8067 - dense_1_loss: 0.4915 - dropout_loss: 0.6304 - val_loss: 0.7409 - val_dense_1_loss: 0.5601 - val_dropout_loss: 0.2315
Epoch 2/5

1/3 [=========>....................] - ETA: 0s - loss: 0.8048 - dense_1_loss: 0.4937 - dropout_loss: 0.6221
3/3 [==============================] - 0s 10ms/step - loss: 0.7674 - dense_1_loss: 0.5114 - dropout_loss: 0.5120 - val_loss: 0.4465 - val_dense_1_loss: 0.4880 - val_dropout_loss: 0.1742
Epoch 3/5

1/3 [=========>....................] - ETA: 0s - loss: 0.7389 - dense_1_loss: 0.4466 - dropout_loss: 0.5845
3/3 [==============================] - 0s 10ms/step - loss: 0.6846 - dense_1_loss: 0.4304 - dropout_loss: 0.5084 - val_loss: 0.4716 - val_dense_1_loss: 0.5430 - val_dropout_loss: 0.1210
Epoch 4/5

1/3 [=========>....................] - ETA: 0s - loss: 0.7108 - dense_1_loss: 0.5678 - dropout_loss: 0.2859
3/3 [==============================] - 0s 10ms/step - loss: 0.7624 - dense_1_loss: 0.5300 - dropout_loss: 0.4648 - val_loss: 0.7397 - val_dense_1_loss: 0.5440 - val_dropout_loss: 0.1611
Epoch 5/5

1/3 [=========>....................] - ETA: 0s - loss: 0.6038 - dense_1_loss: 0.3436 - dropout_loss: 0.5205
3/3 [==============================] - 0s 42ms/step - loss: 0.7172 - dense_1_loss: 0.5088 - dropout_loss: 0.4170 - val_loss: 0.6144 - val_dense_1_loss: 0.5194 - val_dropout_loss: 0.1565
Epoch 1/5

1/3 [=========>....................] - ETA: 0s - loss: 0.9976 - dense_1_loss: 0.6953 - dropout_loss: 0.6045
3/3 [==============================] - 0s 16ms/step - loss: 0.8412 - dense_1_loss: 0.5443 - dropout_loss: 0.5938 - val_loss: 0.6513 - val_dense_1_loss: 0.4147 - val_dropout_loss: 0.1269
Epoch 2/5

1/3 [=========>....................] - ETA: 0s - loss: 1.0820 - dense_1_loss: 0.6186 - dropout_loss: 0.9267
3/3 [==============================] - 0s 10ms/step - loss: 0.9346 - dense_1_loss: 0.6266 - dropout_loss: 0.6159 - val_loss: 0.4080 - val_dense_1_loss: 0.4111 - val_dropout_loss: 0.1842
Epoch 3/5

1/3 [=========>....................] - ETA: 0s - loss: 0.4012 - dense_1_loss: 0.2566 - dropout_loss: 0.2892
3/3 [==============================] - 0s 10ms/step - loss: 0.6741 - dense_1_loss: 0.4948 - dropout_loss: 0.3586 - val_loss: 0.3915 - val_dense_1_loss: 0.3594 - val_dropout_loss: 0.1131
Epoch 4/5

1/3 [=========>....................] - ETA: 0s - loss: 0.6932 - dense_1_loss: 0.5086 - dropout_loss: 0.3692
3/3 [==============================] - 0s 10ms/step - loss: 0.6885 - dense_1_loss: 0.5115 - dropout_loss: 0.3540 - val_loss: 0.4971 - val_dense_1_loss: 0.4994 - val_dropout_loss: 0.1388
Epoch 5/5

1/3 [=========>....................] - ETA: 0s - loss: 1.0218 - dense_1_loss: 0.5591 - dropout_loss: 0.9254
3/3 [==============================] - 0s 42ms/step - loss: 0.7877 - dense_1_loss: 0.4589 - dropout_loss: 0.6576 - val_loss: 0.6956 - val_dense_1_loss: 0.4012 - val_dropout_loss: 0.2139
Epoch 1/5

 1/12 [=>............................] - ETA: 0s - loss: 0.7866 - dense_1_loss: 0.5887 - dropout_loss: 0.3958
10/12 [========================>.....] - ETA: 0s - loss: 0.5782 - dense_1_loss: 0.4198 - dropout_loss: 0.3169
12/12 [==============================] - 0s 12ms/step - loss: 0.6078 - dense_1_loss: 0.4239 - dropout_loss: 0.3679 - val_loss: 0.5964 - val_dense_1_loss: 0.4317 - val_dropout_loss: 0.1481
Epoch 2/5

 1/12 [=>............................] - ETA: 0s - loss: 0.7479 - dense_1_loss: 0.5740 - dropout_loss: 0.3478
 8/12 [===================>..........] - ETA: 0s - loss: 0.7394 - dense_1_loss: 0.4879 - dropout_loss: 0.5031
12/12 [==============================] - 0s 13ms/step - loss: 0.7201 - dense_1_loss: 0.4563 - dropout_loss: 0.5275 - val_loss: 0.5823 - val_dense_1_loss: 0.4624 - val_dropout_loss: 0.1416
Epoch 3/5

 1/12 [=>............................] - ETA: 0s - loss: 0.7842 - dense_1_loss: 0.5083 - dropout_loss: 0.5518
 8/12 [===================>..........] - ETA: 0s - loss: 0.7183 - dense_1_loss: 0.5009 - dropout_loss: 0.4349
12/12 [==============================] - 0s 13ms/step - loss: 0.6656 - dense_1_loss: 0.4504 - dropout_loss: 0.4304 - val_loss: 0.4114 - val_dense_1_loss: 0.3889 - val_dropout_loss: 0.1586
Epoch 4/5

 1/12 [=>............................] - ETA: 0s - loss: 0.7078 - dense_1_loss: 0.3734 - dropout_loss: 0.6688
 9/12 [=====================>........] - ETA: 0s - loss: 0.5873 - dense_1_loss: 0.3432 - dropout_loss: 0.4881
12/12 [==============================] - 0s 12ms/step - loss: 0.5788 - dense_1_loss: 0.3456 - dropout_loss: 0.4664 - val_loss: 0.4150 - val_dense_1_loss: 0.4040 - val_dropout_loss: 0.1738
Epoch 5/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4387 - dense_1_loss: 0.2105 - dropout_loss: 0.4564
 9/12 [=====================>........] - ETA: 0s - loss: 0.6464 - dense_1_loss: 0.3872 - dropout_loss: 0.5185
12/12 [==============================] - 0s 12ms/step - loss: 0.6311 - dense_1_loss: 0.3733 - dropout_loss: 0.5155 - val_loss: 0.4010 - val_dense_1_loss: 0.3279 - val_dropout_loss: 0.1826
Epoch 1/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4787 - dense_1_loss: 0.2797 - dropout_loss: 0.3980
10/12 [========================>.....] - ETA: 0s - loss: 0.5968 - dense_1_loss: 0.3337 - dropout_loss: 0.5262
12/12 [==============================] - 0s 13ms/step - loss: 0.6226 - dense_1_loss: 0.3354 - dropout_loss: 0.5743 - val_loss: 0.4630 - val_dense_1_loss: 0.3479 - val_dropout_loss: 0.1639
Epoch 2/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4830 - dense_1_loss: 0.1990 - dropout_loss: 0.5679
 8/12 [===================>..........] - ETA: 0s - loss: 0.5129 - dense_1_loss: 0.2806 - dropout_loss: 0.4647
12/12 [==============================] - 0s 13ms/step - loss: 0.5011 - dense_1_loss: 0.2622 - dropout_loss: 0.4780 - val_loss: 0.3154 - val_dense_1_loss: 0.2932 - val_dropout_loss: 0.1713
Epoch 3/5

 1/12 [=>............................] - ETA: 0s - loss: 0.5161 - dense_1_loss: 0.2969 - dropout_loss: 0.4385
 8/12 [===================>..........] - ETA: 0s - loss: 0.5501 - dense_1_loss: 0.2958 - dropout_loss: 0.5087
12/12 [==============================] - 0s 13ms/step - loss: 0.5338 - dense_1_loss: 0.2972 - dropout_loss: 0.4731 - val_loss: 0.4027 - val_dense_1_loss: 0.2911 - val_dropout_loss: 0.1660
Epoch 4/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4475 - dense_1_loss: 0.3089 - dropout_loss: 0.2773
 8/12 [===================>..........] - ETA: 0s - loss: 0.5149 - dense_1_loss: 0.2604 - dropout_loss: 0.5089
12/12 [==============================] - 0s 13ms/step - loss: 0.5452 - dense_1_loss: 0.2774 - dropout_loss: 0.5356 - val_loss: 0.4191 - val_dense_1_loss: 0.3155 - val_dropout_loss: 0.1425
Epoch 5/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4760 - dense_1_loss: 0.1058 - dropout_loss: 0.7405
 8/12 [===================>..........] - ETA: 0s - loss: 0.4830 - dense_1_loss: 0.2548 - dropout_loss: 0.4563
12/12 [==============================] - 0s 13ms/step - loss: 0.4728 - dense_1_loss: 0.2539 - dropout_loss: 0.4378 - val_loss: 0.2803 - val_dense_1_loss: 0.2513 - val_dropout_loss: 0.1560
Epoch 1/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4526 - dense_1_loss: 0.2203 - dropout_loss: 0.4645
 9/12 [=====================>........] - ETA: 0s - loss: 0.4360 - dense_1_loss: 0.2529 - dropout_loss: 0.3662
12/12 [==============================] - 0s 12ms/step - loss: 0.4572 - dense_1_loss: 0.2618 - dropout_loss: 0.3907 - val_loss: 0.2986 - val_dense_1_loss: 0.2602 - val_dropout_loss: 0.1735
Epoch 2/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4937 - dense_1_loss: 0.3450 - dropout_loss: 0.2974
 9/12 [=====================>........] - ETA: 0s - loss: 0.4824 - dense_1_loss: 0.2116 - dropout_loss: 0.5415
12/12 [==============================] - 0s 12ms/step - loss: 0.4815 - dense_1_loss: 0.2162 - dropout_loss: 0.5305 - val_loss: 0.3435 - val_dense_1_loss: 0.2328 - val_dropout_loss: 0.1747
Epoch 3/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4005 - dense_1_loss: 0.2170 - dropout_loss: 0.3670
 9/12 [=====================>........] - ETA: 0s - loss: 0.4644 - dense_1_loss: 0.2530 - dropout_loss: 0.4229
12/12 [==============================] - 0s 12ms/step - loss: 0.4638 - dense_1_loss: 0.2570 - dropout_loss: 0.4136 - val_loss: 0.3911 - val_dense_1_loss: 0.2250 - val_dropout_loss: 0.1530
Epoch 4/5

 1/12 [=>............................] - ETA: 0s - loss: 0.6288 - dense_1_loss: 0.3071 - dropout_loss: 0.6435
10/12 [========================>.....] - ETA: 0s - loss: 0.5642 - dense_1_loss: 0.2352 - dropout_loss: 0.6581
12/12 [==============================] - 0s 10ms/step - loss: 0.5366 - dense_1_loss: 0.2366 - dropout_loss: 0.5999 - val_loss: 0.1818 - val_dense_1_loss: 0.1760 - val_dropout_loss: 0.1580
Epoch 5/5

 1/12 [=>............................] - ETA: 0s - loss: 0.3811 - dense_1_loss: 0.1652 - dropout_loss: 0.4317
 9/12 [=====================>........] - ETA: 0s - loss: 0.4885 - dense_1_loss: 0.2247 - dropout_loss: 0.5276
12/12 [==============================] - 0s 12ms/step - loss: 0.4972 - dense_1_loss: 0.2388 - dropout_loss: 0.5169 - val_loss: 0.4682 - val_dense_1_loss: 0.2275 - val_dropout_loss: 0.1744
---------------------------- Captured stderr call -----------------------------
2020-10-04 14:37:48.870405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:73:00.0
2020-10-04 14:37:48.871002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-04 14:37:48.871344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2020-10-04 14:37:48.871686: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2020-10-04 14:37:48.872021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2020-10-04 14:37:48.872361: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2020-10-04 14:37:48.872704: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2020-10-04 14:37:48.873053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-04 14:37:48.873651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-04 14:37:48.873973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-04 14:37:48.874327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-10-04 14:37:48.874548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-10-04 14:37:48.875065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8686 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:73:00.0, compute capability: 7.5)
============================== warnings summary ===============================
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\_pytest\config\__init__.py:1040
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\_pytest\config\__init__.py:1040: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: flaky
    self._mark_plugins_for_rewrite(hook)

test_training.py::test_model_methods
test_training.py::test_model_with_partial_loss
test_training.py::test_model_with_external_loss
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_utils.py:819: UserWarning: Output dense_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_1.
    'be expecting any data to be passed to {0}.'.format(name))

test_training.py::test_model_methods
test_training.py::test_model_with_external_loss
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_utils.py:819: UserWarning: Output dropout missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dropout.
    'be expecting any data to be passed to {0}.'.format(name))

test_training.py::test_model_with_external_loss
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_utils.py:819: UserWarning: Output dense_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_2.
    'be expecting any data to be passed to {0}.'.format(name))

-- Docs: https://docs.pytest.org/en/stable/warnings.html
===Flaky Test Report===

test_model_methods passed 1 out of the required 1 times. Success!
test_fit_generator failed and was not selected for rerun.
	<class 'AttributeError'>
	'str' object has no attribute 'stop'
	[<TracebackEntry C:\Users\mutation\Desktop\testcase\tests\keras\engine\test_training.py:541>, <TracebackEntry C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\legacy\interfaces.py:91>, <TracebackEntry C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training.py:1732>, <TracebackEntry C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_generator.py:287>]

===End Flaky Test Report===
=========================== short test summary info ===========================
FAILED test_training.py::test_fit_generator - AttributeError: 'str' object ha...
============ 1 failed, 32 passed, 1 skipped, 7 warnings in 29.25s =============
