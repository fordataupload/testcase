2020-10-04 14:38:14.641469: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
============================= test session starts =============================
platform win32 -- Python 3.6.12, pytest-6.0.2, py-1.9.0, pluggy-0.13.1
rootdir: C:\Users\mutation\Desktop\testcase\tests\keras\engine
plugins: flaky-3.7.0
collected 34 items

test_training.py ...FF..s.......F.........F........                      [100%]

================================== FAILURES ===================================
_____________________________ test_model_methods ______________________________

    @flaky(rerun_filter=lambda err, *args: issubclass(err[0], AssertionError))
    def test_model_methods():
        model = get_model(num_outputs=2)
    
        optimizer = 'rmsprop'
        loss = 'mse'
        loss_weights = [1., 0.5]
    
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        # training/testing doesn't work before compiling.
        with pytest.raises(RuntimeError):
            model.train_on_batch([input_a_np, input_b_np],
                                 [output_a_np, output_b_np])
    
        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,
                      sample_weight_mode=None)
    
        # test train_on_batch
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                   [output_a_np, output_b_np])
        out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                   {'dense_1': output_a_np, 'dropout': output_b_np})
    
        # test fit
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np], epochs=1, batch_size=4)
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        [output_a_np, output_b_np], epochs=1, batch_size=4)
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        {'dense_1': output_a_np, 'dropout': output_b_np},
                        epochs=1, batch_size=4)
    
        # test validation_split
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4, validation_split=0.5)
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4, validation_split=0.5)
    
        # test validation data
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4,
                        validation_data=([input_a_np, input_b_np],
                                         [output_a_np, output_b_np]))
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4, validation_split=0.5,
                        validation_data=({'input_a': input_a_np,
                                          'input_b': input_b_np},
                                         [output_a_np, output_b_np]))
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        {'dense_1': output_a_np, 'dropout': output_b_np},
                        epochs=1, batch_size=4, validation_split=0.5,
                        validation_data=(
                            {'input_a': input_a_np, 'input_b': input_b_np},
                            {'dense_1': output_a_np, 'dropout': output_b_np}))
    
        # test_on_batch
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                  [output_a_np, output_b_np])
        out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                  {'dense_1': output_a_np, 'dropout': output_b_np})
    
        # predict_on_batch
        out = model.predict_on_batch([input_a_np, input_b_np])
        out = model.predict_on_batch({'input_a': input_a_np,
                                      'input_b': input_b_np})
    
        # predict, evaluate
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        out = model.evaluate([input_a_np, input_b_np],
                             [output_a_np, output_b_np],
                             batch_size=4)
        out = model.predict([input_a_np, input_b_np], batch_size=4)
    
        # with sample_weight
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        sample_weight = [None, np.random.random((10,))]
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np],
                                   sample_weight=sample_weight)
    
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np],
                                  sample_weight=sample_weight)
    
        # test accuracy metric
        model.compile(optimizer, loss, metrics=['acc'],
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        assert len(out) == 5
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == 5
    
        # this should also work
        model.compile(optimizer, loss, metrics={'dense_1': 'acc'},
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        assert len(out) == 4
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == 4
    
        # and this as well
        model.compile(optimizer, loss, metrics={'dense_1': ['acc']},
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        assert len(out) == 4
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == 4
    
        tracker_cb = TrackerCallback()
    
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np], epochs=5, batch_size=4,
                        initial_epoch=2, callbacks=[tracker_cb])
        assert tracker_cb.trained_epochs == [2, 3, 4]
    
        # test starting from non-zero initial epoch for generator too
        tracker_cb = TrackerCallback()
    
        @threadsafe_generator
        def gen_data(batch_sz):
            while True:
                yield ([np.random.random((batch_sz, 3)),
                        np.random.random((batch_sz, 3))],
                       [np.random.random((batch_sz, 4)),
                        np.random.random((batch_sz, 3))])
    
        out = model.fit_generator(gen_data(4), steps_per_epoch=3, epochs=5,
>                                 initial_epoch=2, callbacks=[tracker_cb])

test_training.py:322: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\legacy\interfaces.py:91: in wrapper
    return func(*args, **kwargs)
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training.py:1732: in fit_generator
    initial_epoch=initial_epoch)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = <keras.engine.training.Model object at 0x000001FD2355D240>
generator = <test_training.threadsafe_iter object at 0x000001FD2A0D0748>
steps_per_epoch = 3, epochs = 5, verbose = 1
callbacks = <keras.callbacks.callbacks.CallbackList object at 0x000001FD2A0DDE10>
validation_data = None, validation_steps = None, validation_freq = 1
class_weight = None, max_queue_size = 10, workers = 1
use_multiprocessing = False, shuffle = True, initial_epoch = 2

    def fit_generator(model,
                      generator,
                      steps_per_epoch=None,
                      epochs=1,
                      verbose=1,
                      callbacks=None,
                      validation_data=None,
                      validation_steps=None,
                      validation_freq=1,
                      class_weight=None,
                      max_queue_size=10,
                      workers=1,
                      use_multiprocessing=False,
                      shuffle=True,
                      initial_epoch=0):
        """See docstring for `Model.fit_generator`."""
        epoch = initial_epoch
    
        do_validation = bool(validation_data)
        model._make_train_function()
        if do_validation:
            model._make_test_function()
    
        use_sequence_api = is_sequence(generator)
        if not use_sequence_api and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the `keras.utils.Sequence'
                            ' class.'))
    
        # if generator is instance of Sequence and steps_per_epoch are not provided -
        # recompute steps_per_epoch after each epoch
        recompute_steps_per_epoch = use_sequence_api and steps_per_epoch is None
    
        if steps_per_epoch is None:
            if use_sequence_api:
                steps_per_epoch = len(generator)
            else:
                raise ValueError('`steps_per_epoch=None` is only valid for a'
                                 ' generator based on the '
                                 '`keras.utils.Sequence`'
                                 ' class. Please specify `steps_per_epoch` '
                                 'or use the `keras.utils.Sequence` class.')
    
        # python 2 has 'next', 3 has '__next__'
        # avoid any explicit version checks
        val_use_sequence_api = is_sequence(validation_data)
        val_gen = (hasattr(validation_data, 'next') or
                   hasattr(validation_data, '__next__') or
                   val_use_sequence_api)
        if (val_gen and not val_use_sequence_api and
                not validation_steps):
            raise ValueError('`validation_steps=None` is only valid for a'
                             ' generator based on the `keras.utils.Sequence`'
                             ' class. Please specify `validation_steps` or use'
                             ' the `keras.utils.Sequence` class.')
    
        # Prepare display labels.
        out_labels = model.metrics_names
        callback_metrics = out_labels + ['val_' + n for n in out_labels]
    
        # prepare callbacks
        model.history = cbks.History()
        _callbacks = [cbks.BaseLogger(
            stateful_metrics=model.metrics_names[1:])]
        if verbose:
            _callbacks.append(
                cbks.ProgbarLogger(
                    count_mode='steps',
                    stateful_metrics=model.metrics_names[1:]))
        _callbacks += (callbacks or []) + [model.history]
        callbacks = cbks.CallbackList(_callbacks)
    
        # it's possible to callback a different model than self:
        callback_model = model._get_callback_model()
    
        callbacks.set_model(callback_model)
        callbacks.set_params({
            'epochs': epochs,
            'steps': steps_per_epoch,
            'verbose': verbose,
            'do_validation': do_validation,
            'metrics': callback_metrics,
        })
        callbacks._call_begin_hook('train')
    
        enqueuer = None
        val_enqueuer = ""
    
        try:
            if do_validation:
                if val_gen and workers > 0:
                    # Create an Enqueuer that can be reused
                    val_data = validation_data
                    if is_sequence(val_data):
                        val_enqueuer = OrderedEnqueuer(
                            val_data,
                            use_multiprocessing=use_multiprocessing)
                        validation_steps = validation_steps or len(val_data)
                    else:
                        val_enqueuer = GeneratorEnqueuer(
                            val_data,
                            use_multiprocessing=use_multiprocessing)
                    val_enqueuer.start(workers=workers,
                                       max_queue_size=max_queue_size)
                    val_enqueuer_gen = val_enqueuer.get()
                elif val_gen:
                    val_data = validation_data
                    if is_sequence(val_data):
                        val_enqueuer_gen = iter_sequence_infinite(val_data)
                        validation_steps = validation_steps or len(val_data)
                    else:
                        val_enqueuer_gen = val_data
                else:
                    # Prepare data for validation
                    if len(validation_data) == 2:
                        val_x, val_y = validation_data
                        val_sample_weight = None
                    elif len(validation_data) == 3:
                        val_x, val_y, val_sample_weight = validation_data
                    else:
                        raise ValueError('`validation_data` should be a tuple '
                                         '`(val_x, val_y, val_sample_weight)` '
                                         'or `(val_x, val_y)`. Found: ' +
                                         str(validation_data))
                    val_x, val_y, val_sample_weights = model._standardize_user_data(
                        val_x, val_y, val_sample_weight)
                    val_data = val_x + val_y + val_sample_weights
                    if model.uses_learning_phase and not isinstance(K.learning_phase(),
                                                                    int):
                        val_data += [0.]
                    for cbk in callbacks:
                        cbk.validation_data = val_data
    
            if workers > 0:
                if use_sequence_api:
                    enqueuer = OrderedEnqueuer(
                        generator,
                        use_multiprocessing=use_multiprocessing,
                        shuffle=shuffle)
                else:
                    enqueuer = GeneratorEnqueuer(
                        generator,
                        use_multiprocessing=use_multiprocessing)
                enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                output_generator = enqueuer.get()
            else:
                if use_sequence_api:
                    output_generator = iter_sequence_infinite(generator)
                else:
                    output_generator = generator
    
            callbacks.model.stop_training = False
            # Construct epoch logs.
            epoch_logs = {}
            while epoch < epochs:
                model.reset_metrics()
                callbacks.on_epoch_begin(epoch)
                steps_done = 0
                batch_index = 0
                while steps_done < steps_per_epoch:
                    generator_output = next(output_generator)
    
                    if not hasattr(generator_output, '__len__'):
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
    
                    if len(generator_output) == 2:
                        x, y = generator_output
                        sample_weight = None
                    elif len(generator_output) == 3:
                        x, y, sample_weight = generator_output
                    else:
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
                    if x is None or len(x) == 0:
                        # Handle data tensors support when no input given
                        # step-size = 1 for data tensors
                        batch_size = 1
                    elif isinstance(x, list):
                        batch_size = x[0].shape[0]
                    elif isinstance(x, dict):
                        batch_size = list(x.values())[0].shape[0]
                    else:
                        batch_size = x.shape[0]
                    # build batch logs
                    batch_logs = {'batch': batch_index, 'size': batch_size}
                    callbacks.on_batch_begin(batch_index, batch_logs)
    
                    outs = model.train_on_batch(x, y,
                                                sample_weight=sample_weight,
                                                class_weight=class_weight,
                                                reset_metrics=False)
    
                    outs = to_list(outs)
                    for l, o in zip(out_labels, outs):
                        batch_logs[l] = o
    
                    callbacks._call_batch_hook('train', 'end', batch_index, batch_logs)
    
                    batch_index += 1
                    steps_done += 1
    
                    # Epoch finished.
                    if (steps_done >= steps_per_epoch and
                            do_validation and
                            should_run_validation(validation_freq, epoch)):
                        # Note that `callbacks` here is an instance of
                        # `keras.callbacks.CallbackList`
                        if val_gen:
                            val_outs = model.evaluate_generator(
                                val_enqueuer_gen,
                                validation_steps,
                                callbacks=callbacks,
                                workers=0)
                        else:
                            # No need for try/except because
                            # data has already been validated.
                            val_outs = model.evaluate(
                                val_x, val_y,
                                batch_size=batch_size,
                                sample_weight=val_sample_weights,
                                callbacks=callbacks,
                                verbose=0)
                        val_outs = to_list(val_outs)
                        # Same labels assumed.
                        for l, o in zip(out_labels, val_outs):
                            epoch_logs['val_' + l] = o
    
                    if callbacks.model.stop_training:
                        break
    
                callbacks.on_epoch_end(epoch, epoch_logs)
                epoch += 1
                if callbacks.model.stop_training:
                    break
    
                if use_sequence_api and workers == 0:
                    generator.on_epoch_end()
    
                if recompute_steps_per_epoch:
                    if workers > 0:
                        enqueuer.join_end_of_epoch()
    
                    # recomute steps per epochs in case if Sequence changes it's length
                    steps_per_epoch = len(generator)
    
                    # update callbacks to make sure params are valid each epoch
                    callbacks.set_params({
                        'epochs': epochs,
                        'steps': steps_per_epoch,
                        'verbose': verbose,
                        'do_validation': do_validation,
                        'metrics': callback_metrics,
                    })
    
        finally:
            try:
                if enqueuer is not None:
                    enqueuer.stop()
            finally:
                if val_enqueuer is not None:
>                   val_enqueuer.stop()
E                   AttributeError: 'str' object has no attribute 'stop'

C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_generator.py:290: AttributeError
---------------------------- Captured stdout call -----------------------------
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.7587 - dense_1_loss: 0.3704 - dropout_loss: 0.7766
10/10 [==============================] - 0s 2ms/step - loss: 0.5902 - dense_1_loss: 0.3089 - dropout_loss: 0.5361
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.6425 - dense_1_loss: 0.2669 - dropout_loss: 0.7513
10/10 [==============================] - 0s 2ms/step - loss: 0.6476 - dense_1_loss: 0.2778 - dropout_loss: 0.7265
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.4638 - dense_1_loss: 0.2553 - dropout_loss: 0.4171
10/10 [==============================] - 0s 0us/step - loss: 0.5673 - dense_1_loss: 0.2961 - dropout_loss: 0.5423
Train on 5 samples, validate on 5 samples
Epoch 1/1

4/5 [=======================>......] - ETA: 0s - loss: 0.7520 - dense_1_loss: 0.4102 - dropout_loss: 0.6836
5/5 [==============================] - 0s 9ms/step - loss: 0.7355 - dense_1_loss: 0.4125 - dropout_loss: 0.5966 - val_loss: 0.2168 - val_dense_1_loss: 0.1946 - val_dropout_loss: 0.1217
Train on 5 samples, validate on 5 samples
Epoch 1/1

4/5 [=======================>......] - ETA: 0s - loss: 0.6405 - dense_1_loss: 0.4046 - dropout_loss: 0.4719
5/5 [==============================] - 0s 3ms/step - loss: 0.6371 - dense_1_loss: 0.4040 - dropout_loss: 0.4557 - val_loss: 0.2158 - val_dense_1_loss: 0.1927 - val_dropout_loss: 0.1217
Train on 10 samples, validate on 10 samples
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.5554 - dense_1_loss: 0.2286 - dropout_loss: 0.6535
10/10 [==============================] - 0s 3ms/step - loss: 0.5918 - dense_1_loss: 0.2748 - dropout_loss: 0.6497 - val_loss: 0.3480 - val_dense_1_loss: 0.2629 - val_dropout_loss: 0.1384
Train on 10 samples, validate on 10 samples
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.3382 - dense_1_loss: 0.1591 - dropout_loss: 0.3583
10/10 [==============================] - 0s 3ms/step - loss: 0.5362 - dense_1_loss: 0.2886 - dropout_loss: 0.5085 - val_loss: 0.3445 - val_dense_1_loss: 0.2598 - val_dropout_loss: 0.1384
Train on 10 samples, validate on 10 samples
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.4711 - dense_1_loss: 0.2729 - dropout_loss: 0.3965
10/10 [==============================] - 0s 3ms/step - loss: 0.5392 - dense_1_loss: 0.2585 - dropout_loss: 0.5555 - val_loss: 0.3412 - val_dense_1_loss: 0.2569 - val_dropout_loss: 0.1384

 4/10 [===========>..................] - ETA: 0s
10/10 [==============================] - 0s 2ms/step
Epoch 3/5

 4/10 [===========>..................] - ETA: 0s - loss: 0.7353 - dense_1_loss: 0.2300 - dropout_loss: 0.5053 - dense_1_acc: 0.7500
10/10 [==============================] - 0s 2ms/step - loss: 0.8242 - dense_1_loss: 0.2705 - dropout_loss: 0.5778 - dense_1_acc: 0.5000
Epoch 4/5

 4/10 [===========>..................] - ETA: 0s - loss: 0.9644 - dense_1_loss: 0.4121 - dropout_loss: 0.5523 - dense_1_acc: 0.2500
10/10 [==============================] - 0s 2ms/step - loss: 0.8563 - dense_1_loss: 0.2375 - dropout_loss: 0.6151 - dense_1_acc: 0.4000
Epoch 5/5

 4/10 [===========>..................] - ETA: 0s - loss: 1.0225 - dense_1_loss: 0.2196 - dropout_loss: 0.8030 - dense_1_acc: 0.5000
10/10 [==============================] - 0s 2ms/step - loss: 0.9358 - dense_1_loss: 0.2616 - dropout_loss: 0.6639 - dense_1_acc: 0.4000
Epoch 3/5

1/3 [=========>....................] - ETA: 0s - loss: 0.6720 - dense_1_loss: 0.1413 - dropout_loss: 0.5307 - dense_1_acc: 0.5000
3/3 [==============================] - 0s 10ms/step - loss: 0.5986 - dense_1_loss: 0.1880 - dropout_loss: 0.4106 - dense_1_acc: 0.1667
Epoch 4/5

1/3 [=========>....................] - ETA: 0s - loss: 0.4355 - dense_1_loss: 0.1793 - dropout_loss: 0.2562 - dense_1_acc: 0.0000e+00
3/3 [==============================] - 0s 10ms/step - loss: 0.6136 - dense_1_loss: 0.2312 - dropout_loss: 0.3823 - dense_1_acc: 0.0833
Epoch 5/5

1/3 [=========>....................] - ETA: 0s - loss: 0.7677 - dense_1_loss: 0.2122 - dropout_loss: 0.5555 - dense_1_acc: 0.5000
3/3 [==============================] - 0s 10ms/step - loss: 0.7653 - dense_1_loss: 0.2515 - dropout_loss: 0.5138 - dense_1_acc: 0.5000
---------------------------- Captured stderr call -----------------------------
Using TensorFlow backend.
WARNING:tensorflow:From C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2020-10-04 14:38:17.004142: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-10-04 14:38:17.135557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:73:00.0
2020-10-04 14:38:17.136891: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-04 14:38:17.140493: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2020-10-04 14:38:17.143894: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2020-10-04 14:38:17.145560: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2020-10-04 14:38:17.150323: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2020-10-04 14:38:17.153520: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2020-10-04 14:38:17.162953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-04 14:38:17.163962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-04 14:38:17.164581: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-10-04 14:38:17.178833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:73:00.0
2020-10-04 14:38:17.179385: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-04 14:38:17.179732: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2020-10-04 14:38:17.180076: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2020-10-04 14:38:17.180418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2020-10-04 14:38:17.180775: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2020-10-04 14:38:17.181122: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2020-10-04 14:38:17.181473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-04 14:38:17.182369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-04 14:38:18.134253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-04 14:38:18.134671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-10-04 14:38:18.134891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-10-04 14:38:18.135635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8686 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:73:00.0, compute capability: 7.5)
WARNING:tensorflow:From C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

2020-10-04 14:38:18.763328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
------------------------------ Captured log call ------------------------------
WARNING  tensorflow:deprecation.py:506 From C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING  tensorflow:module_wrapper.py:139 From C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.
_____________________________ test_fit_generator ______________________________

    @flaky(rerun_filter=lambda err, *args: issubclass(err[0], AssertionError))
    def test_fit_generator():
        model = get_model(num_outputs=2)
        optimizer = 'rmsprop'
        loss = 'mse'
        loss_weights = [1., 0.5]
    
        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,
                      sample_weight_mode=None)
        tracker_cb = TrackerCallback()
        val_seq = RandomSequence(4)
        out = model.fit_generator(generator=RandomSequence(3),
                                  steps_per_epoch=3,
                                  epochs=5,
                                  initial_epoch=0,
                                  validation_data=val_seq,
                                  validation_steps=3,
                                  max_queue_size=1,
                                  callbacks=[tracker_cb])
        assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
        assert tracker_cb.trained_batches == list(range(3)) * 5
        assert len(val_seq.logs) <= 4 * 5
    
        tracker_cb = TrackerCallback()
        val_seq = RandomSequence(4)
        out = model.fit(RandomSequence(3),
                        steps_per_epoch=3,
                        epochs=5,
                        initial_epoch=0,
                        validation_data=val_seq,
                        validation_steps=3,
                        max_queue_size=1,
                        callbacks=[tracker_cb])
        assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
        assert tracker_cb.trained_batches == list(range(3)) * 5
        assert len(val_seq.logs) <= 4 * 5
    
        # steps_per_epoch will be equal to len of sequence if it's unspecified
        tracker_cb = TrackerCallback()
        val_seq = RandomSequence(4)
        out = model.fit_generator(generator=RandomSequence(3),
                                  epochs=5,
                                  initial_epoch=0,
                                  validation_data=val_seq,
                                  callbacks=[tracker_cb],
                                  max_queue_size=1)
        assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
        assert tracker_cb.trained_batches == list(range(12)) * 5
        assert 12 * 5 <= len(val_seq.logs) <= (12 * 5) + 2  # the queue may be full.
    
        tracker_cb = TrackerCallback()
        val_seq = RandomSequence(4)
        out = model.fit(RandomSequence(3),
                        epochs=5,
                        initial_epoch=0,
                        validation_data=val_seq,
                        callbacks=[tracker_cb],
                        max_queue_size=1)
        assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
        assert tracker_cb.trained_batches == list(range(12)) * 5
        assert 12 * 5 <= len(val_seq.logs) <= (12 * 5) + 2  # the queue may be full.
    
        # test for workers = 0
        tracker_cb = TrackerCallback()
        val_seq = RandomSequence(4)
        out = model.fit_generator(generator=RandomSequence(3),
                                  epochs=5,
                                  validation_data=val_seq,
                                  callbacks=[tracker_cb],
>                                 workers=0)

test_training.py:541: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\legacy\interfaces.py:91: in wrapper
    return func(*args, **kwargs)
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training.py:1732: in fit_generator
    initial_epoch=initial_epoch)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = <keras.engine.training.Model object at 0x000001FFB3D64CC0>
generator = <test_training.RandomSequence object at 0x000001FFB3B54860>
steps_per_epoch = 12, epochs = 5, verbose = 1
callbacks = <keras.callbacks.callbacks.CallbackList object at 0x000001FFB3B63A90>
validation_data = <test_training.RandomSequence object at 0x000001FFB3B63748>
validation_steps = 12, validation_freq = 1, class_weight = None
max_queue_size = 10, workers = 0, use_multiprocessing = False, shuffle = True
initial_epoch = 0

    def fit_generator(model,
                      generator,
                      steps_per_epoch=None,
                      epochs=1,
                      verbose=1,
                      callbacks=None,
                      validation_data=None,
                      validation_steps=None,
                      validation_freq=1,
                      class_weight=None,
                      max_queue_size=10,
                      workers=1,
                      use_multiprocessing=False,
                      shuffle=True,
                      initial_epoch=0):
        """See docstring for `Model.fit_generator`."""
        epoch = initial_epoch
    
        do_validation = bool(validation_data)
        model._make_train_function()
        if do_validation:
            model._make_test_function()
    
        use_sequence_api = is_sequence(generator)
        if not use_sequence_api and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the `keras.utils.Sequence'
                            ' class.'))
    
        # if generator is instance of Sequence and steps_per_epoch are not provided -
        # recompute steps_per_epoch after each epoch
        recompute_steps_per_epoch = use_sequence_api and steps_per_epoch is None
    
        if steps_per_epoch is None:
            if use_sequence_api:
                steps_per_epoch = len(generator)
            else:
                raise ValueError('`steps_per_epoch=None` is only valid for a'
                                 ' generator based on the '
                                 '`keras.utils.Sequence`'
                                 ' class. Please specify `steps_per_epoch` '
                                 'or use the `keras.utils.Sequence` class.')
    
        # python 2 has 'next', 3 has '__next__'
        # avoid any explicit version checks
        val_use_sequence_api = is_sequence(validation_data)
        val_gen = (hasattr(validation_data, 'next') or
                   hasattr(validation_data, '__next__') or
                   val_use_sequence_api)
        if (val_gen and not val_use_sequence_api and
                not validation_steps):
            raise ValueError('`validation_steps=None` is only valid for a'
                             ' generator based on the `keras.utils.Sequence`'
                             ' class. Please specify `validation_steps` or use'
                             ' the `keras.utils.Sequence` class.')
    
        # Prepare display labels.
        out_labels = model.metrics_names
        callback_metrics = out_labels + ['val_' + n for n in out_labels]
    
        # prepare callbacks
        model.history = cbks.History()
        _callbacks = [cbks.BaseLogger(
            stateful_metrics=model.metrics_names[1:])]
        if verbose:
            _callbacks.append(
                cbks.ProgbarLogger(
                    count_mode='steps',
                    stateful_metrics=model.metrics_names[1:]))
        _callbacks += (callbacks or []) + [model.history]
        callbacks = cbks.CallbackList(_callbacks)
    
        # it's possible to callback a different model than self:
        callback_model = model._get_callback_model()
    
        callbacks.set_model(callback_model)
        callbacks.set_params({
            'epochs': epochs,
            'steps': steps_per_epoch,
            'verbose': verbose,
            'do_validation': do_validation,
            'metrics': callback_metrics,
        })
        callbacks._call_begin_hook('train')
    
        enqueuer = None
        val_enqueuer = ""
    
        try:
            if do_validation:
                if val_gen and workers > 0:
                    # Create an Enqueuer that can be reused
                    val_data = validation_data
                    if is_sequence(val_data):
                        val_enqueuer = OrderedEnqueuer(
                            val_data,
                            use_multiprocessing=use_multiprocessing)
                        validation_steps = validation_steps or len(val_data)
                    else:
                        val_enqueuer = GeneratorEnqueuer(
                            val_data,
                            use_multiprocessing=use_multiprocessing)
                    val_enqueuer.start(workers=workers,
                                       max_queue_size=max_queue_size)
                    val_enqueuer_gen = val_enqueuer.get()
                elif val_gen:
                    val_data = validation_data
                    if is_sequence(val_data):
                        val_enqueuer_gen = iter_sequence_infinite(val_data)
                        validation_steps = validation_steps or len(val_data)
                    else:
                        val_enqueuer_gen = val_data
                else:
                    # Prepare data for validation
                    if len(validation_data) == 2:
                        val_x, val_y = validation_data
                        val_sample_weight = None
                    elif len(validation_data) == 3:
                        val_x, val_y, val_sample_weight = validation_data
                    else:
                        raise ValueError('`validation_data` should be a tuple '
                                         '`(val_x, val_y, val_sample_weight)` '
                                         'or `(val_x, val_y)`. Found: ' +
                                         str(validation_data))
                    val_x, val_y, val_sample_weights = model._standardize_user_data(
                        val_x, val_y, val_sample_weight)
                    val_data = val_x + val_y + val_sample_weights
                    if model.uses_learning_phase and not isinstance(K.learning_phase(),
                                                                    int):
                        val_data += [0.]
                    for cbk in callbacks:
                        cbk.validation_data = val_data
    
            if workers > 0:
                if use_sequence_api:
                    enqueuer = OrderedEnqueuer(
                        generator,
                        use_multiprocessing=use_multiprocessing,
                        shuffle=shuffle)
                else:
                    enqueuer = GeneratorEnqueuer(
                        generator,
                        use_multiprocessing=use_multiprocessing)
                enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                output_generator = enqueuer.get()
            else:
                if use_sequence_api:
                    output_generator = iter_sequence_infinite(generator)
                else:
                    output_generator = generator
    
            callbacks.model.stop_training = False
            # Construct epoch logs.
            epoch_logs = {}
            while epoch < epochs:
                model.reset_metrics()
                callbacks.on_epoch_begin(epoch)
                steps_done = 0
                batch_index = 0
                while steps_done < steps_per_epoch:
                    generator_output = next(output_generator)
    
                    if not hasattr(generator_output, '__len__'):
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
    
                    if len(generator_output) == 2:
                        x, y = generator_output
                        sample_weight = None
                    elif len(generator_output) == 3:
                        x, y, sample_weight = generator_output
                    else:
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
                    if x is None or len(x) == 0:
                        # Handle data tensors support when no input given
                        # step-size = 1 for data tensors
                        batch_size = 1
                    elif isinstance(x, list):
                        batch_size = x[0].shape[0]
                    elif isinstance(x, dict):
                        batch_size = list(x.values())[0].shape[0]
                    else:
                        batch_size = x.shape[0]
                    # build batch logs
                    batch_logs = {'batch': batch_index, 'size': batch_size}
                    callbacks.on_batch_begin(batch_index, batch_logs)
    
                    outs = model.train_on_batch(x, y,
                                                sample_weight=sample_weight,
                                                class_weight=class_weight,
                                                reset_metrics=False)
    
                    outs = to_list(outs)
                    for l, o in zip(out_labels, outs):
                        batch_logs[l] = o
    
                    callbacks._call_batch_hook('train', 'end', batch_index, batch_logs)
    
                    batch_index += 1
                    steps_done += 1
    
                    # Epoch finished.
                    if (steps_done >= steps_per_epoch and
                            do_validation and
                            should_run_validation(validation_freq, epoch)):
                        # Note that `callbacks` here is an instance of
                        # `keras.callbacks.CallbackList`
                        if val_gen:
                            val_outs = model.evaluate_generator(
                                val_enqueuer_gen,
                                validation_steps,
                                callbacks=callbacks,
                                workers=0)
                        else:
                            # No need for try/except because
                            # data has already been validated.
                            val_outs = model.evaluate(
                                val_x, val_y,
                                batch_size=batch_size,
                                sample_weight=val_sample_weights,
                                callbacks=callbacks,
                                verbose=0)
                        val_outs = to_list(val_outs)
                        # Same labels assumed.
                        for l, o in zip(out_labels, val_outs):
                            epoch_logs['val_' + l] = o
    
                    if callbacks.model.stop_training:
                        break
    
                callbacks.on_epoch_end(epoch, epoch_logs)
                epoch += 1
                if callbacks.model.stop_training:
                    break
    
                if use_sequence_api and workers == 0:
                    generator.on_epoch_end()
    
                if recompute_steps_per_epoch:
                    if workers > 0:
                        enqueuer.join_end_of_epoch()
    
                    # recomute steps per epochs in case if Sequence changes it's length
                    steps_per_epoch = len(generator)
    
                    # update callbacks to make sure params are valid each epoch
                    callbacks.set_params({
                        'epochs': epochs,
                        'steps': steps_per_epoch,
                        'verbose': verbose,
                        'do_validation': do_validation,
                        'metrics': callback_metrics,
                    })
    
        finally:
            try:
                if enqueuer is not None:
                    enqueuer.stop()
            finally:
                if val_enqueuer is not None:
>                   val_enqueuer.stop()
E                   AttributeError: 'str' object has no attribute 'stop'

C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_generator.py:290: AttributeError
---------------------------- Captured stdout call -----------------------------
Epoch 1/5

1/3 [=========>....................] - ETA: 0s - loss: 0.4648 - dense_1_loss: 0.3156 - dropout_loss: 0.2984
3/3 [==============================] - 0s 36ms/step - loss: 0.5372 - dense_1_loss: 0.3171 - dropout_loss: 0.4402 - val_loss: 0.5084 - val_dense_1_loss: 0.3908 - val_dropout_loss: 0.2033
Epoch 2/5

1/3 [=========>....................] - ETA: 0s - loss: 1.0907 - dense_1_loss: 0.6299 - dropout_loss: 0.9216
3/3 [==============================] - 0s 10ms/step - loss: 0.8546 - dense_1_loss: 0.5767 - dropout_loss: 0.5557 - val_loss: 0.4831 - val_dense_1_loss: 0.5051 - val_dropout_loss: 0.2445
Epoch 3/5

1/3 [=========>....................] - ETA: 0s - loss: 0.4343 - dense_1_loss: 0.1715 - dropout_loss: 0.5255
3/3 [==============================] - 0s 10ms/step - loss: 0.5823 - dense_1_loss: 0.2387 - dropout_loss: 0.6873 - val_loss: 0.3252 - val_dense_1_loss: 0.2843 - val_dropout_loss: 0.1846
Epoch 4/5

1/3 [=========>....................] - ETA: 0s - loss: 0.5512 - dense_1_loss: 0.3815 - dropout_loss: 0.3394
3/3 [==============================] - 0s 10ms/step - loss: 0.6436 - dense_1_loss: 0.4495 - dropout_loss: 0.3883 - val_loss: 0.5196 - val_dense_1_loss: 0.3954 - val_dropout_loss: 0.1843
Epoch 5/5

1/3 [=========>....................] - ETA: 0s - loss: 0.6659 - dense_1_loss: 0.4150 - dropout_loss: 0.5018
3/3 [==============================] - 0s 47ms/step - loss: 0.7011 - dense_1_loss: 0.3833 - dropout_loss: 0.6356 - val_loss: 0.2731 - val_dense_1_loss: 0.3033 - val_dropout_loss: 0.1244
Epoch 1/5

1/3 [=========>....................] - ETA: 0s - loss: 0.2681 - dense_1_loss: 0.1326 - dropout_loss: 0.2711
3/3 [==============================] - 0s 10ms/step - loss: 0.6212 - dense_1_loss: 0.2855 - dropout_loss: 0.6712 - val_loss: 0.6146 - val_dense_1_loss: 0.4476 - val_dropout_loss: 0.1606
Epoch 2/5

1/3 [=========>....................] - ETA: 0s - loss: 0.8675 - dense_1_loss: 0.5589 - dropout_loss: 0.6171
3/3 [==============================] - 0s 10ms/step - loss: 0.6713 - dense_1_loss: 0.4327 - dropout_loss: 0.4772 - val_loss: 0.5196 - val_dense_1_loss: 0.3747 - val_dropout_loss: 0.1731
Epoch 3/5

1/3 [=========>....................] - ETA: 0s - loss: 0.9236 - dense_1_loss: 0.5565 - dropout_loss: 0.7342
3/3 [==============================] - 0s 10ms/step - loss: 0.7389 - dense_1_loss: 0.4360 - dropout_loss: 0.6057 - val_loss: 0.5056 - val_dense_1_loss: 0.3208 - val_dropout_loss: 0.1719
Epoch 4/5

1/3 [=========>....................] - ETA: 0s - loss: 0.6777 - dense_1_loss: 0.2522 - dropout_loss: 0.8510
3/3 [==============================] - 0s 16ms/step - loss: 0.6146 - dense_1_loss: 0.3186 - dropout_loss: 0.5921 - val_loss: 0.5484 - val_dense_1_loss: 0.4291 - val_dropout_loss: 0.1228
Epoch 5/5

1/3 [=========>....................] - ETA: 0s - loss: 0.5067 - dense_1_loss: 0.3542 - dropout_loss: 0.3050
3/3 [==============================] - 0s 42ms/step - loss: 0.6222 - dense_1_loss: 0.3314 - dropout_loss: 0.5816 - val_loss: 0.3527 - val_dense_1_loss: 0.2300 - val_dropout_loss: 0.1694
Epoch 1/5

 1/12 [=>............................] - ETA: 0s - loss: 0.6500 - dense_1_loss: 0.2976 - dropout_loss: 0.7049
11/12 [==========================>...] - ETA: 0s - loss: 0.5445 - dense_1_loss: 0.3012 - dropout_loss: 0.4866
12/12 [==============================] - 0s 10ms/step - loss: 0.5393 - dense_1_loss: 0.2916 - dropout_loss: 0.4955 - val_loss: 0.4231 - val_dense_1_loss: 0.3353 - val_dropout_loss: 0.1606
Epoch 2/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4688 - dense_1_loss: 0.1701 - dropout_loss: 0.5973
 9/12 [=====================>........] - ETA: 0s - loss: 0.5755 - dense_1_loss: 0.2839 - dropout_loss: 0.5832
12/12 [==============================] - 0s 12ms/step - loss: 0.5598 - dense_1_loss: 0.2835 - dropout_loss: 0.5526 - val_loss: 0.3923 - val_dense_1_loss: 0.3149 - val_dropout_loss: 0.2054
Epoch 3/5

 1/12 [=>............................] - ETA: 0s - loss: 0.6966 - dense_1_loss: 0.2867 - dropout_loss: 0.8198
 8/12 [===================>..........] - ETA: 0s - loss: 0.4736 - dense_1_loss: 0.2306 - dropout_loss: 0.4860
12/12 [==============================] - 0s 12ms/step - loss: 0.4434 - dense_1_loss: 0.2159 - dropout_loss: 0.4551 - val_loss: 0.5255 - val_dense_1_loss: 0.2828 - val_dropout_loss: 0.1562
Epoch 4/5

 1/12 [=>............................] - ETA: 0s - loss: 0.6629 - dense_1_loss: 0.2503 - dropout_loss: 0.8253
 9/12 [=====================>........] - ETA: 0s - loss: 0.4456 - dense_1_loss: 0.1934 - dropout_loss: 0.5044
12/12 [==============================] - 0s 10ms/step - loss: 0.4611 - dense_1_loss: 0.2154 - dropout_loss: 0.4914 - val_loss: 0.2271 - val_dense_1_loss: 0.2582 - val_dropout_loss: 0.1564
Epoch 5/5

 1/12 [=>............................] - ETA: 0s - loss: 0.6047 - dense_1_loss: 0.2907 - dropout_loss: 0.6280
 9/12 [=====================>........] - ETA: 0s - loss: 0.5558 - dense_1_loss: 0.2469 - dropout_loss: 0.6178
12/12 [==============================] - 0s 12ms/step - loss: 0.5736 - dense_1_loss: 0.2526 - dropout_loss: 0.6421 - val_loss: 0.1622 - val_dense_1_loss: 0.2623 - val_dropout_loss: 0.1929
Epoch 1/5

 1/12 [=>............................] - ETA: 0s - loss: 0.6610 - dense_1_loss: 0.4593 - dropout_loss: 0.4033
10/12 [========================>.....] - ETA: 0s - loss: 0.5645 - dense_1_loss: 0.2655 - dropout_loss: 0.5980
12/12 [==============================] - 0s 12ms/step - loss: 0.5395 - dense_1_loss: 0.2655 - dropout_loss: 0.5480 - val_loss: 0.2450 - val_dense_1_loss: 0.2674 - val_dropout_loss: 0.1807
Epoch 2/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4652 - dense_1_loss: 0.2168 - dropout_loss: 0.4968
 9/12 [=====================>........] - ETA: 0s - loss: 0.4878 - dense_1_loss: 0.2313 - dropout_loss: 0.5131
12/12 [==============================] - 0s 10ms/step - loss: 0.4741 - dense_1_loss: 0.2309 - dropout_loss: 0.4864 - val_loss: 0.2538 - val_dense_1_loss: 0.2168 - val_dropout_loss: 0.1593
Epoch 3/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4972 - dense_1_loss: 0.2165 - dropout_loss: 0.5613
 9/12 [=====================>........] - ETA: 0s - loss: 0.5303 - dense_1_loss: 0.2184 - dropout_loss: 0.6239
12/12 [==============================] - 0s 10ms/step - loss: 0.5140 - dense_1_loss: 0.2250 - dropout_loss: 0.5779 - val_loss: 0.3104 - val_dense_1_loss: 0.2344 - val_dropout_loss: 0.1794
Epoch 4/5

 1/12 [=>............................] - ETA: 0s - loss: 0.3536 - dense_1_loss: 0.1425 - dropout_loss: 0.4221
 8/12 [===================>..........] - ETA: 0s - loss: 0.3981 - dense_1_loss: 0.1970 - dropout_loss: 0.4023
12/12 [==============================] - 0s 10ms/step - loss: 0.4411 - dense_1_loss: 0.2164 - dropout_loss: 0.4493 - val_loss: 0.2849 - val_dense_1_loss: 0.2021 - val_dropout_loss: 0.1804
Epoch 5/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4381 - dense_1_loss: 0.2474 - dropout_loss: 0.3814
 8/12 [===================>..........] - ETA: 0s - loss: 0.4495 - dense_1_loss: 0.1893 - dropout_loss: 0.5205
12/12 [==============================] - 0s 10ms/step - loss: 0.4414 - dense_1_loss: 0.1825 - dropout_loss: 0.5178 - val_loss: 0.1913 - val_dense_1_loss: 0.2003 - val_dropout_loss: 0.1714
Epoch 1/5

 1/12 [=>............................] - ETA: 0s - loss: 0.5967 - dense_1_loss: 0.2748 - dropout_loss: 0.6439
 9/12 [=====================>........] - ETA: 0s - loss: 0.4030 - dense_1_loss: 0.1725 - dropout_loss: 0.4611
12/12 [==============================] - 0s 12ms/step - loss: 0.4235 - dense_1_loss: 0.1704 - dropout_loss: 0.5061 - val_loss: 0.3797 - val_dense_1_loss: 0.1562 - val_dropout_loss: 0.1680
Epoch 2/5

 1/12 [=>............................] - ETA: 0s - loss: 0.5237 - dense_1_loss: 0.2274 - dropout_loss: 0.5925
12/12 [==============================] - 0s 10ms/step - loss: 0.4259 - dense_1_loss: 0.2036 - dropout_loss: 0.4445 - val_loss: 0.3237 - val_dense_1_loss: 0.2081 - val_dropout_loss: 0.1709
Epoch 3/5

 1/12 [=>............................] - ETA: 0s - loss: 0.5737 - dense_1_loss: 0.0906 - dropout_loss: 0.9662
11/12 [==========================>...] - ETA: 0s - loss: 0.4440 - dense_1_loss: 0.1615 - dropout_loss: 0.5652
12/12 [==============================] - 0s 9ms/step - loss: 0.4324 - dense_1_loss: 0.1590 - dropout_loss: 0.5468 - val_loss: 0.2359 - val_dense_1_loss: 0.1683 - val_dropout_loss: 0.1711
Epoch 4/5

 1/12 [=>............................] - ETA: 0s - loss: 0.3100 - dense_1_loss: 0.1194 - dropout_loss: 0.3812
10/12 [========================>.....] - ETA: 0s - loss: 0.3777 - dense_1_loss: 0.1523 - dropout_loss: 0.4507
12/12 [==============================] - 0s 10ms/step - loss: 0.3710 - dense_1_loss: 0.1518 - dropout_loss: 0.4384 - val_loss: 0.2142 - val_dense_1_loss: 0.1498 - val_dropout_loss: 0.1499
Epoch 5/5

 1/12 [=>............................] - ETA: 0s - loss: 0.3322 - dense_1_loss: 0.1451 - dropout_loss: 0.3742
12/12 [==============================] - 0s 10ms/step - loss: 0.3963 - dense_1_loss: 0.1482 - dropout_loss: 0.4961 - val_loss: 0.1660 - val_dense_1_loss: 0.1972 - val_dropout_loss: 0.1652
---------------------------- Captured stderr call -----------------------------
2020-10-04 14:38:22.755657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:73:00.0
2020-10-04 14:38:22.756262: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-04 14:38:22.756613: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2020-10-04 14:38:22.756955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2020-10-04 14:38:22.757292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2020-10-04 14:38:22.757633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2020-10-04 14:38:22.757976: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2020-10-04 14:38:22.758326: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-04 14:38:22.758929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-04 14:38:22.759247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-04 14:38:22.759604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-10-04 14:38:22.759822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-10-04 14:38:22.760344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8686 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:73:00.0, compute capability: 7.5)
________________________ test_model_with_external_loss ________________________

    @pytest.mark.skipif((K.backend() == 'cntk'),
                        reason='cntk does not support external loss yet')
    def test_model_with_external_loss():
        # None loss, only regularization loss.
        a = Input(shape=(3,), name='input_a')
        a_2 = Dense(4, name='dense_1',
                    kernel_regularizer='l1',
                    bias_regularizer='l2')(a)
        dp = Dropout(0.5, name='dropout')
        a_3 = dp(a_2)
    
        model = Model(a, [a_2, a_3])
    
        optimizer = 'rmsprop'
        loss = None
        model.compile(optimizer, loss, metrics=['mae'])
    
        input_a_np = np.random.random((10, 3))
    
        # test train_on_batch
        out = model.train_on_batch(input_a_np, None)
        out = model.test_on_batch(input_a_np, None)
        # fit
        out = model.fit(input_a_np, None)
        # evaluate
        out = model.evaluate(input_a_np, None)
    
        # No dropout, external loss.
        a = Input(shape=(3,), name='input_a')
        a_2 = Dense(4, name='dense_1')(a)
        a_3 = Dense(4, name='dense_2')(a)
    
        model = Model(a, [a_2, a_3])
        model.add_loss(K.mean(a_3 + a_2))
    
        optimizer = 'rmsprop'
        loss = None
        model.compile(optimizer, loss, metrics=['mae'])
    
        # test train_on_batch
        out = model.train_on_batch(input_a_np, None)
        out = model.test_on_batch(input_a_np, None)
        # fit
        out = model.fit(input_a_np, None)
        # evaluate
        out = model.evaluate(input_a_np, None)
    
        # Test fit with no external data at all.
        if K.backend() == 'tensorflow':
            import tensorflow as tf
    
            a = Input(tensor=tf.Variable(input_a_np, dtype=tf.float32))
            a_2 = Dense(4, name='dense_1')(a)
            a_2 = Dropout(0.5, name='dropout')(a_2)
            model = Model(a, a_2)
            model.add_loss(K.mean(a_2))
    
            model.compile(optimizer='rmsprop',
                          loss=None,
                          metrics=['mean_squared_error'])
    
            # test train_on_batch
            out = model.train_on_batch(None, None)
            out = model.test_on_batch(None, None)
            out = model.predict_on_batch(None)
    
            # test fit
            with pytest.raises(ValueError):
                out = model.fit(None, None, epochs=1, batch_size=10)
            out = model.fit(None, None, epochs=1, steps_per_epoch=1)
    
            # define a generator to produce x=None and y=None
            @threadsafe_generator
            def data_tensors_generator():
                while True:
                    yield (None, None)
    
            generator = data_tensors_generator()
    
            # test fit_generator for framework-native data tensors
            out = model.fit_generator(generator, epochs=1,
>                                     steps_per_epoch=3)

test_training.py:1101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\legacy\interfaces.py:91: in wrapper
    return func(*args, **kwargs)
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training.py:1732: in fit_generator
    initial_epoch=initial_epoch)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = <keras.engine.training.Model object at 0x000001FFB7033208>
generator = <test_training.threadsafe_iter object at 0x000001FFB7095DD8>
steps_per_epoch = 3, epochs = 1, verbose = 1
callbacks = <keras.callbacks.callbacks.CallbackList object at 0x000001FFB709DF28>
validation_data = None, validation_steps = None, validation_freq = 1
class_weight = None, max_queue_size = 10, workers = 1
use_multiprocessing = False, shuffle = True, initial_epoch = 0

    def fit_generator(model,
                      generator,
                      steps_per_epoch=None,
                      epochs=1,
                      verbose=1,
                      callbacks=None,
                      validation_data=None,
                      validation_steps=None,
                      validation_freq=1,
                      class_weight=None,
                      max_queue_size=10,
                      workers=1,
                      use_multiprocessing=False,
                      shuffle=True,
                      initial_epoch=0):
        """See docstring for `Model.fit_generator`."""
        epoch = initial_epoch
    
        do_validation = bool(validation_data)
        model._make_train_function()
        if do_validation:
            model._make_test_function()
    
        use_sequence_api = is_sequence(generator)
        if not use_sequence_api and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the `keras.utils.Sequence'
                            ' class.'))
    
        # if generator is instance of Sequence and steps_per_epoch are not provided -
        # recompute steps_per_epoch after each epoch
        recompute_steps_per_epoch = use_sequence_api and steps_per_epoch is None
    
        if steps_per_epoch is None:
            if use_sequence_api:
                steps_per_epoch = len(generator)
            else:
                raise ValueError('`steps_per_epoch=None` is only valid for a'
                                 ' generator based on the '
                                 '`keras.utils.Sequence`'
                                 ' class. Please specify `steps_per_epoch` '
                                 'or use the `keras.utils.Sequence` class.')
    
        # python 2 has 'next', 3 has '__next__'
        # avoid any explicit version checks
        val_use_sequence_api = is_sequence(validation_data)
        val_gen = (hasattr(validation_data, 'next') or
                   hasattr(validation_data, '__next__') or
                   val_use_sequence_api)
        if (val_gen and not val_use_sequence_api and
                not validation_steps):
            raise ValueError('`validation_steps=None` is only valid for a'
                             ' generator based on the `keras.utils.Sequence`'
                             ' class. Please specify `validation_steps` or use'
                             ' the `keras.utils.Sequence` class.')
    
        # Prepare display labels.
        out_labels = model.metrics_names
        callback_metrics = out_labels + ['val_' + n for n in out_labels]
    
        # prepare callbacks
        model.history = cbks.History()
        _callbacks = [cbks.BaseLogger(
            stateful_metrics=model.metrics_names[1:])]
        if verbose:
            _callbacks.append(
                cbks.ProgbarLogger(
                    count_mode='steps',
                    stateful_metrics=model.metrics_names[1:]))
        _callbacks += (callbacks or []) + [model.history]
        callbacks = cbks.CallbackList(_callbacks)
    
        # it's possible to callback a different model than self:
        callback_model = model._get_callback_model()
    
        callbacks.set_model(callback_model)
        callbacks.set_params({
            'epochs': epochs,
            'steps': steps_per_epoch,
            'verbose': verbose,
            'do_validation': do_validation,
            'metrics': callback_metrics,
        })
        callbacks._call_begin_hook('train')
    
        enqueuer = None
        val_enqueuer = ""
    
        try:
            if do_validation:
                if val_gen and workers > 0:
                    # Create an Enqueuer that can be reused
                    val_data = validation_data
                    if is_sequence(val_data):
                        val_enqueuer = OrderedEnqueuer(
                            val_data,
                            use_multiprocessing=use_multiprocessing)
                        validation_steps = validation_steps or len(val_data)
                    else:
                        val_enqueuer = GeneratorEnqueuer(
                            val_data,
                            use_multiprocessing=use_multiprocessing)
                    val_enqueuer.start(workers=workers,
                                       max_queue_size=max_queue_size)
                    val_enqueuer_gen = val_enqueuer.get()
                elif val_gen:
                    val_data = validation_data
                    if is_sequence(val_data):
                        val_enqueuer_gen = iter_sequence_infinite(val_data)
                        validation_steps = validation_steps or len(val_data)
                    else:
                        val_enqueuer_gen = val_data
                else:
                    # Prepare data for validation
                    if len(validation_data) == 2:
                        val_x, val_y = validation_data
                        val_sample_weight = None
                    elif len(validation_data) == 3:
                        val_x, val_y, val_sample_weight = validation_data
                    else:
                        raise ValueError('`validation_data` should be a tuple '
                                         '`(val_x, val_y, val_sample_weight)` '
                                         'or `(val_x, val_y)`. Found: ' +
                                         str(validation_data))
                    val_x, val_y, val_sample_weights = model._standardize_user_data(
                        val_x, val_y, val_sample_weight)
                    val_data = val_x + val_y + val_sample_weights
                    if model.uses_learning_phase and not isinstance(K.learning_phase(),
                                                                    int):
                        val_data += [0.]
                    for cbk in callbacks:
                        cbk.validation_data = val_data
    
            if workers > 0:
                if use_sequence_api:
                    enqueuer = OrderedEnqueuer(
                        generator,
                        use_multiprocessing=use_multiprocessing,
                        shuffle=shuffle)
                else:
                    enqueuer = GeneratorEnqueuer(
                        generator,
                        use_multiprocessing=use_multiprocessing)
                enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                output_generator = enqueuer.get()
            else:
                if use_sequence_api:
                    output_generator = iter_sequence_infinite(generator)
                else:
                    output_generator = generator
    
            callbacks.model.stop_training = False
            # Construct epoch logs.
            epoch_logs = {}
            while epoch < epochs:
                model.reset_metrics()
                callbacks.on_epoch_begin(epoch)
                steps_done = 0
                batch_index = 0
                while steps_done < steps_per_epoch:
                    generator_output = next(output_generator)
    
                    if not hasattr(generator_output, '__len__'):
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
    
                    if len(generator_output) == 2:
                        x, y = generator_output
                        sample_weight = None
                    elif len(generator_output) == 3:
                        x, y, sample_weight = generator_output
                    else:
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
                    if x is None or len(x) == 0:
                        # Handle data tensors support when no input given
                        # step-size = 1 for data tensors
                        batch_size = 1
                    elif isinstance(x, list):
                        batch_size = x[0].shape[0]
                    elif isinstance(x, dict):
                        batch_size = list(x.values())[0].shape[0]
                    else:
                        batch_size = x.shape[0]
                    # build batch logs
                    batch_logs = {'batch': batch_index, 'size': batch_size}
                    callbacks.on_batch_begin(batch_index, batch_logs)
    
                    outs = model.train_on_batch(x, y,
                                                sample_weight=sample_weight,
                                                class_weight=class_weight,
                                                reset_metrics=False)
    
                    outs = to_list(outs)
                    for l, o in zip(out_labels, outs):
                        batch_logs[l] = o
    
                    callbacks._call_batch_hook('train', 'end', batch_index, batch_logs)
    
                    batch_index += 1
                    steps_done += 1
    
                    # Epoch finished.
                    if (steps_done >= steps_per_epoch and
                            do_validation and
                            should_run_validation(validation_freq, epoch)):
                        # Note that `callbacks` here is an instance of
                        # `keras.callbacks.CallbackList`
                        if val_gen:
                            val_outs = model.evaluate_generator(
                                val_enqueuer_gen,
                                validation_steps,
                                callbacks=callbacks,
                                workers=0)
                        else:
                            # No need for try/except because
                            # data has already been validated.
                            val_outs = model.evaluate(
                                val_x, val_y,
                                batch_size=batch_size,
                                sample_weight=val_sample_weights,
                                callbacks=callbacks,
                                verbose=0)
                        val_outs = to_list(val_outs)
                        # Same labels assumed.
                        for l, o in zip(out_labels, val_outs):
                            epoch_logs['val_' + l] = o
    
                    if callbacks.model.stop_training:
                        break
    
                callbacks.on_epoch_end(epoch, epoch_logs)
                epoch += 1
                if callbacks.model.stop_training:
                    break
    
                if use_sequence_api and workers == 0:
                    generator.on_epoch_end()
    
                if recompute_steps_per_epoch:
                    if workers > 0:
                        enqueuer.join_end_of_epoch()
    
                    # recomute steps per epochs in case if Sequence changes it's length
                    steps_per_epoch = len(generator)
    
                    # update callbacks to make sure params are valid each epoch
                    callbacks.set_params({
                        'epochs': epochs,
                        'steps': steps_per_epoch,
                        'verbose': verbose,
                        'do_validation': do_validation,
                        'metrics': callback_metrics,
                    })
    
        finally:
            try:
                if enqueuer is not None:
                    enqueuer.stop()
            finally:
                if val_enqueuer is not None:
>                   val_enqueuer.stop()
E                   AttributeError: 'str' object has no attribute 'stop'

C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_generator.py:290: AttributeError
---------------------------- Captured stdout call -----------------------------
Epoch 1/1

10/10 [==============================] - 0s 0us/step - loss: 0.0552

10/10 [==============================] - 0s 0us/step
Epoch 1/1

10/10 [==============================] - 0s 0us/step - loss: 0.1304

10/10 [==============================] - 0s 0us/step
Epoch 1/1

1/1 [==============================] - 0s 0us/step - loss: -0.2498
Epoch 1/1

1/3 [=========>....................] - ETA: 0s - loss: -0.2317
3/3 [==============================] - 0s 5ms/step - loss: -0.2625
---------------------------- Captured stderr call -----------------------------
2020-10-04 14:38:31.268406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:73:00.0
2020-10-04 14:38:31.269008: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-04 14:38:31.269355: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2020-10-04 14:38:31.269699: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2020-10-04 14:38:31.270038: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2020-10-04 14:38:31.270379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2020-10-04 14:38:31.270722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2020-10-04 14:38:31.271078: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-04 14:38:31.271675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-04 14:38:31.271989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-04 14:38:31.272348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-10-04 14:38:31.272569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-10-04 14:38:31.273079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8686 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:73:00.0, compute capability: 7.5)
____________________________ test_validation_freq _____________________________

    def test_validation_freq():
        model = Sequential([Dense(1)])
        model.compile('sgd', 'mse')
    
        def _gen():
            while True:
                yield np.ones((2, 10)), np.ones((2, 1))
    
        x, y = np.ones((10, 10)), np.ones((10, 1))
    
        class ValCounter(Callback):
    
            def __init__(self):
                self.val_runs = 0
    
            def on_test_begin(self, logs=None):
                self.val_runs += 1
    
        # Test in training_arrays.py
        val_counter = ValCounter()
        model.fit(
            x,
            y,
            batch_size=2,
            epochs=4,
            validation_data=(x, y),
            validation_freq=2,
            callbacks=[val_counter])
        assert val_counter.val_runs == 2
    
        # Test in training_generator.py
        val_counter = ValCounter()
        model.fit_generator(
            _gen(),
            epochs=4,
            steps_per_epoch=5,
            validation_data=(x, y),
            validation_freq=[4, 2, 2, 1],
>           callbacks=[val_counter])

test_training.py:1731: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\legacy\interfaces.py:91: in wrapper
    return func(*args, **kwargs)
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training.py:1732: in fit_generator
    initial_epoch=initial_epoch)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = <keras.engine.sequential.Sequential object at 0x000001FFB5EA9C18>
generator = <generator object test_validation_freq.<locals>._gen at 0x000001FFB3D0D780>
steps_per_epoch = 5, epochs = 4, verbose = 1
callbacks = <keras.callbacks.callbacks.CallbackList object at 0x000001FFB3AC1470>
validation_data = (array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       [1., 1., 1.,...
       [1.],
       [1.],
       [1.],
       [1.],
       [1.],
       [1.],
       [1.],
       [1.],
       [1.]]))
validation_steps = None, validation_freq = [4, 2, 2, 1], class_weight = None
max_queue_size = 10, workers = 1, use_multiprocessing = False, shuffle = True
initial_epoch = 0

    def fit_generator(model,
                      generator,
                      steps_per_epoch=None,
                      epochs=1,
                      verbose=1,
                      callbacks=None,
                      validation_data=None,
                      validation_steps=None,
                      validation_freq=1,
                      class_weight=None,
                      max_queue_size=10,
                      workers=1,
                      use_multiprocessing=False,
                      shuffle=True,
                      initial_epoch=0):
        """See docstring for `Model.fit_generator`."""
        epoch = initial_epoch
    
        do_validation = bool(validation_data)
        model._make_train_function()
        if do_validation:
            model._make_test_function()
    
        use_sequence_api = is_sequence(generator)
        if not use_sequence_api and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the `keras.utils.Sequence'
                            ' class.'))
    
        # if generator is instance of Sequence and steps_per_epoch are not provided -
        # recompute steps_per_epoch after each epoch
        recompute_steps_per_epoch = use_sequence_api and steps_per_epoch is None
    
        if steps_per_epoch is None:
            if use_sequence_api:
                steps_per_epoch = len(generator)
            else:
                raise ValueError('`steps_per_epoch=None` is only valid for a'
                                 ' generator based on the '
                                 '`keras.utils.Sequence`'
                                 ' class. Please specify `steps_per_epoch` '
                                 'or use the `keras.utils.Sequence` class.')
    
        # python 2 has 'next', 3 has '__next__'
        # avoid any explicit version checks
        val_use_sequence_api = is_sequence(validation_data)
        val_gen = (hasattr(validation_data, 'next') or
                   hasattr(validation_data, '__next__') or
                   val_use_sequence_api)
        if (val_gen and not val_use_sequence_api and
                not validation_steps):
            raise ValueError('`validation_steps=None` is only valid for a'
                             ' generator based on the `keras.utils.Sequence`'
                             ' class. Please specify `validation_steps` or use'
                             ' the `keras.utils.Sequence` class.')
    
        # Prepare display labels.
        out_labels = model.metrics_names
        callback_metrics = out_labels + ['val_' + n for n in out_labels]
    
        # prepare callbacks
        model.history = cbks.History()
        _callbacks = [cbks.BaseLogger(
            stateful_metrics=model.metrics_names[1:])]
        if verbose:
            _callbacks.append(
                cbks.ProgbarLogger(
                    count_mode='steps',
                    stateful_metrics=model.metrics_names[1:]))
        _callbacks += (callbacks or []) + [model.history]
        callbacks = cbks.CallbackList(_callbacks)
    
        # it's possible to callback a different model than self:
        callback_model = model._get_callback_model()
    
        callbacks.set_model(callback_model)
        callbacks.set_params({
            'epochs': epochs,
            'steps': steps_per_epoch,
            'verbose': verbose,
            'do_validation': do_validation,
            'metrics': callback_metrics,
        })
        callbacks._call_begin_hook('train')
    
        enqueuer = None
        val_enqueuer = ""
    
        try:
            if do_validation:
                if val_gen and workers > 0:
                    # Create an Enqueuer that can be reused
                    val_data = validation_data
                    if is_sequence(val_data):
                        val_enqueuer = OrderedEnqueuer(
                            val_data,
                            use_multiprocessing=use_multiprocessing)
                        validation_steps = validation_steps or len(val_data)
                    else:
                        val_enqueuer = GeneratorEnqueuer(
                            val_data,
                            use_multiprocessing=use_multiprocessing)
                    val_enqueuer.start(workers=workers,
                                       max_queue_size=max_queue_size)
                    val_enqueuer_gen = val_enqueuer.get()
                elif val_gen:
                    val_data = validation_data
                    if is_sequence(val_data):
                        val_enqueuer_gen = iter_sequence_infinite(val_data)
                        validation_steps = validation_steps or len(val_data)
                    else:
                        val_enqueuer_gen = val_data
                else:
                    # Prepare data for validation
                    if len(validation_data) == 2:
                        val_x, val_y = validation_data
                        val_sample_weight = None
                    elif len(validation_data) == 3:
                        val_x, val_y, val_sample_weight = validation_data
                    else:
                        raise ValueError('`validation_data` should be a tuple '
                                         '`(val_x, val_y, val_sample_weight)` '
                                         'or `(val_x, val_y)`. Found: ' +
                                         str(validation_data))
                    val_x, val_y, val_sample_weights = model._standardize_user_data(
                        val_x, val_y, val_sample_weight)
                    val_data = val_x + val_y + val_sample_weights
                    if model.uses_learning_phase and not isinstance(K.learning_phase(),
                                                                    int):
                        val_data += [0.]
                    for cbk in callbacks:
                        cbk.validation_data = val_data
    
            if workers > 0:
                if use_sequence_api:
                    enqueuer = OrderedEnqueuer(
                        generator,
                        use_multiprocessing=use_multiprocessing,
                        shuffle=shuffle)
                else:
                    enqueuer = GeneratorEnqueuer(
                        generator,
                        use_multiprocessing=use_multiprocessing)
                enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                output_generator = enqueuer.get()
            else:
                if use_sequence_api:
                    output_generator = iter_sequence_infinite(generator)
                else:
                    output_generator = generator
    
            callbacks.model.stop_training = False
            # Construct epoch logs.
            epoch_logs = {}
            while epoch < epochs:
                model.reset_metrics()
                callbacks.on_epoch_begin(epoch)
                steps_done = 0
                batch_index = 0
                while steps_done < steps_per_epoch:
                    generator_output = next(output_generator)
    
                    if not hasattr(generator_output, '__len__'):
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
    
                    if len(generator_output) == 2:
                        x, y = generator_output
                        sample_weight = None
                    elif len(generator_output) == 3:
                        x, y, sample_weight = generator_output
                    else:
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
                    if x is None or len(x) == 0:
                        # Handle data tensors support when no input given
                        # step-size = 1 for data tensors
                        batch_size = 1
                    elif isinstance(x, list):
                        batch_size = x[0].shape[0]
                    elif isinstance(x, dict):
                        batch_size = list(x.values())[0].shape[0]
                    else:
                        batch_size = x.shape[0]
                    # build batch logs
                    batch_logs = {'batch': batch_index, 'size': batch_size}
                    callbacks.on_batch_begin(batch_index, batch_logs)
    
                    outs = model.train_on_batch(x, y,
                                                sample_weight=sample_weight,
                                                class_weight=class_weight,
                                                reset_metrics=False)
    
                    outs = to_list(outs)
                    for l, o in zip(out_labels, outs):
                        batch_logs[l] = o
    
                    callbacks._call_batch_hook('train', 'end', batch_index, batch_logs)
    
                    batch_index += 1
                    steps_done += 1
    
                    # Epoch finished.
                    if (steps_done >= steps_per_epoch and
                            do_validation and
                            should_run_validation(validation_freq, epoch)):
                        # Note that `callbacks` here is an instance of
                        # `keras.callbacks.CallbackList`
                        if val_gen:
                            val_outs = model.evaluate_generator(
                                val_enqueuer_gen,
                                validation_steps,
                                callbacks=callbacks,
                                workers=0)
                        else:
                            # No need for try/except because
                            # data has already been validated.
                            val_outs = model.evaluate(
                                val_x, val_y,
                                batch_size=batch_size,
                                sample_weight=val_sample_weights,
                                callbacks=callbacks,
                                verbose=0)
                        val_outs = to_list(val_outs)
                        # Same labels assumed.
                        for l, o in zip(out_labels, val_outs):
                            epoch_logs['val_' + l] = o
    
                    if callbacks.model.stop_training:
                        break
    
                callbacks.on_epoch_end(epoch, epoch_logs)
                epoch += 1
                if callbacks.model.stop_training:
                    break
    
                if use_sequence_api and workers == 0:
                    generator.on_epoch_end()
    
                if recompute_steps_per_epoch:
                    if workers > 0:
                        enqueuer.join_end_of_epoch()
    
                    # recomute steps per epochs in case if Sequence changes it's length
                    steps_per_epoch = len(generator)
    
                    # update callbacks to make sure params are valid each epoch
                    callbacks.set_params({
                        'epochs': epochs,
                        'steps': steps_per_epoch,
                        'verbose': verbose,
                        'do_validation': do_validation,
                        'metrics': callback_metrics,
                    })
    
        finally:
            try:
                if enqueuer is not None:
                    enqueuer.stop()
            finally:
                if val_enqueuer is not None:
>                   val_enqueuer.stop()
E                   AttributeError: 'str' object has no attribute 'stop'

C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_generator.py:290: AttributeError
---------------------------- Captured stdout call -----------------------------
Train on 10 samples, validate on 10 samples
Epoch 1/4

 2/10 [=====>........................] - ETA: 0s - loss: 0.0235
10/10 [==============================] - 0s 8ms/step - loss: 0.0110
Epoch 2/4

 2/10 [=====>........................] - ETA: 0s - loss: 0.0020
10/10 [==============================] - 0s 5ms/step - loss: 9.1879e-04 - val_loss: 1.6360e-04
Epoch 3/4

 2/10 [=====>........................] - ETA: 0s - loss: 1.6360e-04
10/10 [==============================] - 0s 3ms/step - loss: 7.6587e-05
Epoch 4/4

 2/10 [=====>........................] - ETA: 0s - loss: 1.3635e-05
10/10 [==============================] - 0s 3ms/step - loss: 6.3835e-06 - val_loss: 1.1360e-06
Epoch 1/4

1/5 [=====>........................] - ETA: 0s - loss: 1.1360e-06
5/5 [==============================] - 0s 6ms/step - loss: 5.3192e-07 - val_loss: 9.4740e-08
Epoch 2/4

1/5 [=====>........................] - ETA: 0s - loss: 9.4740e-08
5/5 [==============================] - 0s 6ms/step - loss: 4.4344e-08 - val_loss: 7.8874e-09
Epoch 3/4

1/5 [=====>........................] - ETA: 0s - loss: 7.8874e-09
5/5 [==============================] - 0s 6ms/step - loss: 3.6944e-09 - val_loss: 7.8874e-09
Epoch 4/4

1/5 [=====>........................] - ETA: 0s - loss: 6.6917e-10
5/5 [==============================] - 0s 6ms/step - loss: 3.0844e-10 - val_loss: 5.6403e-11
---------------------------- Captured stderr call -----------------------------
2020-10-04 14:38:41.623685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:73:00.0
2020-10-04 14:38:41.624290: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-04 14:38:41.624659: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2020-10-04 14:38:41.625005: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2020-10-04 14:38:41.625341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2020-10-04 14:38:41.625684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2020-10-04 14:38:41.626027: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2020-10-04 14:38:41.626370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-04 14:38:41.626943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-04 14:38:41.627256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-04 14:38:41.627625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-10-04 14:38:41.627849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-10-04 14:38:41.628393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8686 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:73:00.0, compute capability: 7.5)
============================== warnings summary ===============================
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\_pytest\config\__init__.py:1040
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\_pytest\config\__init__.py:1040: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: flaky
    self._mark_plugins_for_rewrite(hook)

test_training.py::test_model_with_partial_loss
test_training.py::test_model_with_external_loss
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_utils.py:819: UserWarning: Output dense_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_1.
    'be expecting any data to be passed to {0}.'.format(name))

test_training.py::test_model_with_external_loss
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_utils.py:819: UserWarning: Output dropout missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dropout.
    'be expecting any data to be passed to {0}.'.format(name))

test_training.py::test_model_with_external_loss
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_utils.py:819: UserWarning: Output dense_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_2.
    'be expecting any data to be passed to {0}.'.format(name))

-- Docs: https://docs.pytest.org/en/stable/warnings.html
===Flaky Test Report===

test_model_methods failed and was not selected for rerun.
	<class 'AttributeError'>
	'str' object has no attribute 'stop'
	[<TracebackEntry C:\Users\mutation\Desktop\testcase\tests\keras\engine\test_training.py:322>, <TracebackEntry C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\legacy\interfaces.py:91>, <TracebackEntry C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training.py:1732>, <TracebackEntry C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_generator.py:290>]
test_fit_generator failed and was not selected for rerun.
	<class 'AttributeError'>
	'str' object has no attribute 'stop'
	[<TracebackEntry C:\Users\mutation\Desktop\testcase\tests\keras\engine\test_training.py:541>, <TracebackEntry C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\legacy\interfaces.py:91>, <TracebackEntry C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training.py:1732>, <TracebackEntry C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_generator.py:290>]

===End Flaky Test Report===
=========================== short test summary info ===========================
FAILED test_training.py::test_model_methods - AttributeError: 'str' object ha...
FAILED test_training.py::test_fit_generator - AttributeError: 'str' object ha...
FAILED test_training.py::test_model_with_external_loss - AttributeError: 'str...
FAILED test_training.py::test_validation_freq - AttributeError: 'str' object ...
============ 4 failed, 29 passed, 1 skipped, 5 warnings in 27.65s =============
