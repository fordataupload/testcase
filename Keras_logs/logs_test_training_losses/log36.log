2020-10-04 13:20:14.181192: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
============================= test session starts =============================
platform win32 -- Python 3.6.12, pytest-6.0.2, py-1.9.0, pluggy-0.13.1
rootdir: C:\Users\mutation\Desktop\testcase\tests\keras\engine
plugins: flaky-3.7.0
collected 34 items

test_training.py .......s..............F...........                      [100%]

================================== FAILURES ===================================
_____________ test_model_with_crossentropy_losses_channels_first ______________

    def test_model_with_crossentropy_losses_channels_first():
        """Tests use of all crossentropy losses with `channels_first`.
    
        Tests `sparse_categorical_crossentropy`, `categorical_crossentropy`,
        and `binary_crossentropy`.
        Verifies that evaluate gives the same result with either
        `channels_first` or `channels_last` image_data_format.
        Tests PR #9715.
        """
    
        def prepare_simple_model(input_tensor, loss_name, target):
            axis = 1 if K.image_data_format() == 'channels_first' else -1
            if loss_name == 'sparse_categorical_crossentropy':
                loss = lambda y_true, y_pred: K.sparse_categorical_crossentropy(
                    y_true, y_pred, axis=axis)
                num_channels = np.amax(target) + 1
                activation = 'softmax'
            elif loss_name == 'categorical_crossentropy':
                loss = lambda y_true, y_pred: K.categorical_crossentropy(
                    y_true, y_pred, axis=axis)
                num_channels = target.shape[axis]
                activation = 'softmax'
            elif loss_name == 'binary_crossentropy':
                loss = lambda y_true, y_pred: K.binary_crossentropy(y_true, y_pred)
                num_channels = target.shape[axis]
                activation = 'sigmoid'
            predictions = Conv2D(num_channels, 1, activation=activation,
                                 kernel_initializer='ones',
                                 bias_initializer='ones')(input_tensor)
            simple_model = Model(inputs=input_tensor, outputs=predictions)
            simple_model.compile(optimizer='rmsprop', loss=loss)
            return simple_model
    
        losses_to_test = ['sparse_categorical_crossentropy',
                          'categorical_crossentropy', 'binary_crossentropy']
    
        data_channels_first = np.array([[[[8., 7.1, 0.], [4.5, 2.6, 0.55],
                                          [0.9, 4.2, 11.2]]]], dtype=np.float32)
        # Labels for testing 4-class sparse_categorical_crossentropy, 4-class
        # categorical_crossentropy, and 2-class binary_crossentropy:
        labels_channels_first = [np.array([[[[0, 1, 3], [2, 1, 0], [2, 2, 1]]]]),
                                 np.array([[[[0, 1, 0], [0, 1, 0], [0, 0, 0]],
                                            [[1, 0, 0], [0, 0, 1], [0, 1, 0]],
                                            [[0, 0, 0], [1, 0, 0], [0, 0, 1]],
                                            [[0, 0, 1], [0, 0, 0], [1, 0, 0]]]]),
                                 np.array([[[[0, 1, 0], [0, 1, 0], [0, 0, 1]],
                                            [[1, 0, 1], [1, 0, 1], [1, 1, 0]]]])]
        # Compute one loss for each loss function in the list `losses_to_test`:
        loss_channels_last = [0., 0., 0.]
        loss_channels_first = [0., 0., 0.]
    
        old_data_format = K.image_data_format()
    
        # Evaluate a simple network with channels last, with all three loss
        # functions:
        K.set_image_data_format('channels_last')
        data = np.moveaxis(data_channels_first, 1, -1)
        for index, loss_function in enumerate(losses_to_test):
            labels = np.moveaxis(labels_channels_first[index], 1, -1)
            inputs = Input(shape=(3, 3, 1))
>           model = prepare_simple_model(inputs, loss_function, labels)

test_training.py:1614: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
test_training.py:1584: in prepare_simple_model
    simple_model.compile(optimizer='rmsprop', loss=loss)
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training.py:229: in compile
    self.total_loss = self._prepare_total_loss(masks)
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training.py:692: in _prepare_total_loss
    y_true, y_pred, sample_weight=sample_weight)
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\losses.py:70: in __call__
    with K.name_scope(scope_name):
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\tensorflow_core\python\framework\ops.py:6346: in __enter__
    return self._name_scope.__enter__()
C:\ProgramData\Anaconda3\envs\keras\lib\contextlib.py:81: in __enter__
    return next(self.gen)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tensorflow.python.framework.ops.Graph object at 0x00000251ACA3AC18>
name = '<lambda>'

    @tf_contextlib.contextmanager
    def name_scope(self, name):
      """Returns a context manager that creates hierarchical names for operations.
    
      A graph maintains a stack of name scopes. A `with name_scope(...):`
      statement pushes a new name onto the stack for the lifetime of the context.
    
      The `name` argument will be interpreted as follows:
    
      * A string (not ending with '/') will create a new name scope, in which
        `name` is appended to the prefix of all operations created in the
        context. If `name` has been used before, it will be made unique by
        calling `self.unique_name(name)`.
      * A scope previously captured from a `with g.name_scope(...) as
        scope:` statement will be treated as an "absolute" name scope, which
        makes it possible to re-enter existing scopes.
      * A value of `None` or the empty string will reset the current name scope
        to the top-level (empty) name scope.
    
      For example:
    
      ```python
      with tf.Graph().as_default() as g:
        c = tf.constant(5.0, name="c")
        assert c.op.name == "c"
        c_1 = tf.constant(6.0, name="c")
        assert c_1.op.name == "c_1"
    
        # Creates a scope called "nested"
        with g.name_scope("nested") as scope:
          nested_c = tf.constant(10.0, name="c")
          assert nested_c.op.name == "nested/c"
    
          # Creates a nested scope called "inner".
          with g.name_scope("inner"):
            nested_inner_c = tf.constant(20.0, name="c")
            assert nested_inner_c.op.name == "nested/inner/c"
    
          # Create a nested scope called "inner_1".
          with g.name_scope("inner"):
            nested_inner_1_c = tf.constant(30.0, name="c")
            assert nested_inner_1_c.op.name == "nested/inner_1/c"
    
            # Treats `scope` as an absolute name scope, and
            # switches to the "nested/" scope.
            with g.name_scope(scope):
              nested_d = tf.constant(40.0, name="d")
              assert nested_d.op.name == "nested/d"
    
              with g.name_scope(""):
                e = tf.constant(50.0, name="e")
                assert e.op.name == "e"
      ```
    
      The name of the scope itself can be captured by `with
      g.name_scope(...) as scope:`, which stores the name of the scope
      in the variable `scope`. This value can be used to name an
      operation that represents the overall result of executing the ops
      in a scope. For example:
    
      ```python
      inputs = tf.constant(...)
      with g.name_scope('my_layer') as scope:
        weights = tf.Variable(..., name="weights")
        biases = tf.Variable(..., name="biases")
        affine = tf.matmul(inputs, weights) + biases
        output = tf.nn.relu(affine, name=scope)
      ```
    
      NOTE: This constructor validates the given `name`. Valid scope
      names match one of the following regular expressions:
    
          [A-Za-z0-9.][A-Za-z0-9_.\\-/]* (for scopes at the root)
          [A-Za-z0-9_.\\-/]* (for other scopes)
    
      Args:
        name: A name for the scope.
    
      Returns:
        A context manager that installs `name` as a new name scope.
    
      Raises:
        ValueError: If `name` is not a valid scope name, according to the rules
          above.
      """
      if name:
        if isinstance(name, compat.bytes_or_text_types):
          name = compat.as_str(name)
    
        if self._name_stack:
          # Scopes created in a nested scope may have initial characters
          # that are illegal as the initial character of an op name
          # (viz. '-', '\', '/', and '_').
          if not _VALID_SCOPE_NAME_REGEX.match(name):
>           raise ValueError("'%s' is not a valid scope name" % name)
E           ValueError: '<lambda>' is not a valid scope name

C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\tensorflow_core\python\framework\ops.py:4124: ValueError
============================== warnings summary ===============================
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\_pytest\config\__init__.py:1040
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\_pytest\config\__init__.py:1040: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: flaky
    self._mark_plugins_for_rewrite(hook)

test_training.py::test_model_methods
test_training.py::test_model_with_partial_loss
test_training.py::test_model_with_external_loss
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_utils.py:819: UserWarning: Output dense_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_1.
    'be expecting any data to be passed to {0}.'.format(name))

test_training.py::test_model_methods
test_training.py::test_model_with_external_loss
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_utils.py:819: UserWarning: Output dropout missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dropout.
    'be expecting any data to be passed to {0}.'.format(name))

test_training.py::test_model_with_external_loss
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_utils.py:819: UserWarning: Output dense_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_2.
    'be expecting any data to be passed to {0}.'.format(name))

-- Docs: https://docs.pytest.org/en/stable/warnings.html
===Flaky Test Report===

test_model_methods passed 1 out of the required 1 times. Success!
test_fit_generator passed 1 out of the required 1 times. Success!

===End Flaky Test Report===
=========================== short test summary info ===========================
FAILED test_training.py::test_model_with_crossentropy_losses_channels_first
============ 1 failed, 32 passed, 1 skipped, 7 warnings in 28.79s =============
