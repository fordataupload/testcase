2020-10-04 14:44:16.817690: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
============================= test session starts =============================
platform win32 -- Python 3.6.12, pytest-6.0.2, py-1.9.0, pluggy-0.13.1
rootdir: C:\Users\mutation\Desktop\testcase\tests\keras\engine
plugins: flaky-3.7.0
collected 34 items

test_training.py ...FF..s..........................                      [100%]

================================== FAILURES ===================================
_____________________________ test_model_methods ______________________________

    @flaky(rerun_filter=lambda err, *args: issubclass(err[0], AssertionError))
    def test_model_methods():
        model = get_model(num_outputs=2)
    
        optimizer = 'rmsprop'
        loss = 'mse'
        loss_weights = [1., 0.5]
    
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        # training/testing doesn't work before compiling.
        with pytest.raises(RuntimeError):
            model.train_on_batch([input_a_np, input_b_np],
                                 [output_a_np, output_b_np])
    
        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,
                      sample_weight_mode=None)
    
        # test train_on_batch
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                   [output_a_np, output_b_np])
        out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                   {'dense_1': output_a_np, 'dropout': output_b_np})
    
        # test fit
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np], epochs=1, batch_size=4)
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        [output_a_np, output_b_np], epochs=1, batch_size=4)
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        {'dense_1': output_a_np, 'dropout': output_b_np},
                        epochs=1, batch_size=4)
    
        # test validation_split
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4, validation_split=0.5)
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4, validation_split=0.5)
    
        # test validation data
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4,
                        validation_data=([input_a_np, input_b_np],
                                         [output_a_np, output_b_np]))
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4, validation_split=0.5,
                        validation_data=({'input_a': input_a_np,
                                          'input_b': input_b_np},
                                         [output_a_np, output_b_np]))
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        {'dense_1': output_a_np, 'dropout': output_b_np},
                        epochs=1, batch_size=4, validation_split=0.5,
                        validation_data=(
                            {'input_a': input_a_np, 'input_b': input_b_np},
                            {'dense_1': output_a_np, 'dropout': output_b_np}))
    
        # test_on_batch
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                  [output_a_np, output_b_np])
        out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                  {'dense_1': output_a_np, 'dropout': output_b_np})
    
        # predict_on_batch
        out = model.predict_on_batch([input_a_np, input_b_np])
        out = model.predict_on_batch({'input_a': input_a_np,
                                      'input_b': input_b_np})
    
        # predict, evaluate
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        out = model.evaluate([input_a_np, input_b_np],
                             [output_a_np, output_b_np],
                             batch_size=4)
        out = model.predict([input_a_np, input_b_np], batch_size=4)
    
        # with sample_weight
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        sample_weight = [None, np.random.random((10,))]
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np],
                                   sample_weight=sample_weight)
    
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np],
                                  sample_weight=sample_weight)
    
        # test accuracy metric
        model.compile(optimizer, loss, metrics=['acc'],
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        assert len(out) == 5
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == 5
    
        # this should also work
        model.compile(optimizer, loss, metrics={'dense_1': 'acc'},
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        assert len(out) == 4
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == 4
    
        # and this as well
        model.compile(optimizer, loss, metrics={'dense_1': ['acc']},
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        assert len(out) == 4
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == 4
    
        tracker_cb = TrackerCallback()
    
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np], epochs=5, batch_size=4,
                        initial_epoch=2, callbacks=[tracker_cb])
        assert tracker_cb.trained_epochs == [2, 3, 4]
    
        # test starting from non-zero initial epoch for generator too
        tracker_cb = TrackerCallback()
    
        @threadsafe_generator
        def gen_data(batch_sz):
            while True:
                yield ([np.random.random((batch_sz, 3)),
                        np.random.random((batch_sz, 3))],
                       [np.random.random((batch_sz, 4)),
                        np.random.random((batch_sz, 3))])
    
        out = model.fit_generator(gen_data(4), steps_per_epoch=3, epochs=5,
                                  initial_epoch=2, callbacks=[tracker_cb])
        assert tracker_cb.trained_epochs == [2, 3, 4]
    
        # test with a custom metric function
        def mse(y_true, y_pred):
            return K.mean(K.pow(y_true - y_pred, 2))
    
        model.compile(optimizer, loss, metrics=[mse],
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        out_len = 1 + 2 * (1 + 1)  # total loss + 2 outputs * (loss + metric)
        assert len(out) == out_len
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == out_len
    
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np],
                        batch_size=4, epochs=1)
        out = model.evaluate([input_a_np, input_b_np],
                             [output_a_np, output_b_np],
                             batch_size=4)
        out = model.predict([input_a_np, input_b_np], batch_size=4)
    
        # enable verbose for evaluate_generator
        out = model.evaluate_generator(gen_data(4), steps=3, verbose=1)
        # pass generator directly so `is_generator_or_sequence`
        # doesn't get confused.
        out = model.evaluate(gen_data(4).it, steps=3, verbose=1)
    
        # empty batch
        with pytest.raises(ValueError):
            @threadsafe_generator
            def gen_data():
                while True:
                    yield (np.asarray([]), np.asarray([]))
    
>           out = model.evaluate_generator(gen_data(), steps=1)

test_training.py:367: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\legacy\interfaces.py:91: in wrapper
    return func(*args, **kwargs)
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training.py:1791: in evaluate_generator
    verbose=verbose)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = <keras.engine.training.Model object at 0x000002251836C208>
generator = <test_training.threadsafe_iter object at 0x000002255E820CF8>
steps = 1
callbacks = <keras.callbacks.callbacks.CallbackList object at 0x000002255E820C88>
max_queue_size = 10, workers = 1, use_multiprocessing = False, verbose = 0

    def evaluate_generator(model, generator,
                           steps=None,
                           callbacks=None,
                           max_queue_size=10,
                           workers=1,
                           use_multiprocessing=False,
                           verbose=0):
        """See docstring for `Model.evaluate_generator`."""
        model._make_test_function()
        model.reset_metrics()
    
        steps_done = 1
        outs_per_batch = []
        batch_sizes = []
        use_sequence_api = is_sequence(generator)
        if not use_sequence_api and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the `keras.utils.Sequence'
                            ' class.'))
        if steps is None:
            if use_sequence_api:
                steps = len(generator)
            else:
                raise ValueError('`steps=None` is only valid for a generator'
                                 ' based on the `keras.utils.Sequence` class.'
                                 ' Please specify `steps` or use the'
                                 ' `keras.utils.Sequence` class.')
        enqueuer = None
    
        # Check if callbacks have not been already configured
        if not isinstance(callbacks, cbks.CallbackList):
            callbacks = cbks.CallbackList(callbacks)
            callback_model = model._get_callback_model()
            callbacks.set_model(callback_model)
            callback_metrics = list(model.metrics_names)
            callback_params = {
                'steps': steps,
                'verbose': verbose,
                'metrics': callback_metrics,
            }
            callbacks.set_params(callback_params)
    
        callbacks.model.stop_training = False
        callbacks._call_begin_hook('test')
    
        try:
            if workers > 0:
                if use_sequence_api:
                    enqueuer = OrderedEnqueuer(
                        generator,
                        use_multiprocessing=use_multiprocessing)
                else:
                    enqueuer = GeneratorEnqueuer(
                        generator,
                        use_multiprocessing=use_multiprocessing)
                enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                output_generator = enqueuer.get()
            else:
                if use_sequence_api:
                    output_generator = iter_sequence_infinite(generator)
                else:
                    output_generator = generator
    
            if verbose == 1:
                progbar = Progbar(target=steps)
    
            while steps_done < steps:
                generator_output = next(output_generator)
                if not hasattr(generator_output, '__len__'):
                    raise ValueError('Output of generator should be a tuple '
                                     '(x, y, sample_weight) '
                                     'or (x, y). Found: ' +
                                     str(generator_output))
                if len(generator_output) == 2:
                    x, y = generator_output
                    sample_weight = None
                elif len(generator_output) == 3:
                    x, y, sample_weight = generator_output
                else:
                    raise ValueError('Output of generator should be a tuple '
                                     '(x, y, sample_weight) '
                                     'or (x, y). Found: ' +
                                     str(generator_output))
    
                if x is None or len(x) == 0:
                    # Handle data tensors support when no input given
                    # step-size = 1 for data tensors
                    batch_size = 1
                elif isinstance(x, list):
                    batch_size = x[0].shape[0]
                elif isinstance(x, dict):
                    batch_size = list(x.values())[0].shape[0]
                else:
                    batch_size = x.shape[0]
                if batch_size == 0:
                    raise ValueError('Received an empty batch. '
                                     'Batches should contain '
                                     'at least one item.')
    
                batch_logs = {'batch': steps_done, 'size': batch_size}
                callbacks._call_batch_hook('test', 'begin', steps_done, batch_logs)
                outs = model.test_on_batch(x, y,
                                           sample_weight=sample_weight,
                                           reset_metrics=False)
                outs = to_list(outs)
                outs_per_batch.append(outs)
    
                for l, o in zip(model.metrics_names, outs):
                    batch_logs[l] = o
                callbacks._call_batch_hook('test', 'end', steps_done, batch_logs)
    
                steps_done += 1
                batch_sizes.append(batch_size)
    
                if verbose == 1:
                    progbar.update(steps_done)
            callbacks._call_end_hook('test')
    
        finally:
            if enqueuer is not None:
                enqueuer.stop()
    
>       averages = [float(outs_per_batch[-1][0])]  # index 0 = 'loss'
E       IndexError: list index out of range

C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_generator.py:420: IndexError
---------------------------- Captured stdout call -----------------------------
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.3711 - dense_1_loss: 0.1662 - dropout_loss: 0.4098
10/10 [==============================] - 0s 2ms/step - loss: 0.6107 - dense_1_loss: 0.4010 - dropout_loss: 0.5334
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.4115 - dense_1_loss: 0.1772 - dropout_loss: 0.4686
10/10 [==============================] - 0s 2ms/step - loss: 0.6126 - dense_1_loss: 0.3974 - dropout_loss: 0.5030
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.4309 - dense_1_loss: 0.1829 - dropout_loss: 0.4960
10/10 [==============================] - 0s 2ms/step - loss: 0.5640 - dense_1_loss: 0.3262 - dropout_loss: 0.3954
Train on 5 samples, validate on 5 samples
Epoch 1/1

4/5 [=======================>......] - ETA: 0s - loss: 0.7830 - dense_1_loss: 0.4955 - dropout_loss: 0.5749
5/5 [==============================] - 0s 9ms/step - loss: 0.6858 - dense_1_loss: 0.3164 - dropout_loss: 0.4473 - val_loss: 0.2983 - val_dense_1_loss: 0.1929 - val_dropout_loss: 0.0938
Train on 5 samples, validate on 5 samples
Epoch 1/1

4/5 [=======================>......] - ETA: 0s - loss: 0.7979 - dense_1_loss: 0.4814 - dropout_loss: 0.6329
5/5 [==============================] - 0s 3ms/step - loss: 0.7161 - dense_1_loss: 0.3326 - dropout_loss: 0.5219 - val_loss: 0.2978 - val_dense_1_loss: 0.1927 - val_dropout_loss: 0.0938
Train on 10 samples, validate on 10 samples
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.5114 - dense_1_loss: 0.2372 - dropout_loss: 0.5483
10/10 [==============================] - 0s 3ms/step - loss: 0.5671 - dense_1_loss: 0.3054 - dropout_loss: 0.4602 - val_loss: 0.4089 - val_dense_1_loss: 0.3002 - val_dropout_loss: 0.1506
Train on 10 samples, validate on 10 samples
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.5293 - dense_1_loss: 0.2345 - dropout_loss: 0.5896
10/10 [==============================] - 0s 5ms/step - loss: 0.5959 - dense_1_loss: 0.3228 - dropout_loss: 0.5430 - val_loss: 0.4063 - val_dense_1_loss: 0.2981 - val_dropout_loss: 0.1506
Train on 10 samples, validate on 10 samples
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.6811 - dense_1_loss: 0.3041 - dropout_loss: 0.7540
10/10 [==============================] - 0s 3ms/step - loss: 0.5994 - dense_1_loss: 0.2964 - dropout_loss: 0.5058 - val_loss: 0.4042 - val_dense_1_loss: 0.2963 - val_dropout_loss: 0.1506

 4/10 [===========>..................] - ETA: 0s
10/10 [==============================] - 0s 0us/step
Epoch 3/5

 4/10 [===========>..................] - ETA: 0s - loss: 0.5995 - dense_1_loss: 0.2137 - dropout_loss: 0.3858 - dense_1_acc: 0.2500
10/10 [==============================] - 0s 2ms/step - loss: 0.6223 - dense_1_loss: 0.2676 - dropout_loss: 0.3277 - dense_1_acc: 0.1000
Epoch 4/5

 4/10 [===========>..................] - ETA: 0s - loss: 0.5363 - dense_1_loss: 0.2016 - dropout_loss: 0.3347 - dense_1_acc: 0.0000e+00
10/10 [==============================] - 0s 3ms/step - loss: 0.6866 - dense_1_loss: 0.2369 - dropout_loss: 0.4203 - dense_1_acc: 0.1000
Epoch 5/5

 4/10 [===========>..................] - ETA: 0s - loss: 0.4248 - dense_1_loss: 0.1413 - dropout_loss: 0.2835 - dense_1_acc: 0.0000e+00
10/10 [==============================] - 0s 2ms/step - loss: 0.6774 - dense_1_loss: 0.2364 - dropout_loss: 0.4804 - dense_1_acc: 0.1000
Epoch 3/5

1/3 [=========>....................] - ETA: 0s - loss: 0.5825 - dense_1_loss: 0.2970 - dropout_loss: 0.2855 - dense_1_acc: 0.5000
3/3 [==============================] - 0s 10ms/step - loss: 0.7577 - dense_1_loss: 0.2276 - dropout_loss: 0.5301 - dense_1_acc: 0.3333
Epoch 4/5

1/3 [=========>....................] - ETA: 0s - loss: 0.7784 - dense_1_loss: 0.2358 - dropout_loss: 0.5425 - dense_1_acc: 0.2500
3/3 [==============================] - 0s 5ms/step - loss: 0.7952 - dense_1_loss: 0.2442 - dropout_loss: 0.5510 - dense_1_acc: 0.1667
Epoch 5/5

1/3 [=========>....................] - ETA: 0s - loss: 0.8450 - dense_1_loss: 0.3675 - dropout_loss: 0.4775 - dense_1_acc: 0.2500
3/3 [==============================] - 0s 5ms/step - loss: 0.8596 - dense_1_loss: 0.3815 - dropout_loss: 0.4781 - dense_1_acc: 0.1667
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.5351 - dense_1_loss: 0.1925 - dropout_loss: 0.3426 - dense_1_mse: 0.1925 - dropout_mse: 0.3426
10/10 [==============================] - 0s 2ms/step - loss: 0.8196 - dense_1_loss: 0.2581 - dropout_loss: 0.5365 - dense_1_mse: 0.2581 - dropout_mse: 0.5365

 4/10 [===========>..................] - ETA: 0s
10/10 [==============================] - 0s 0us/step

2/3 [===================>..........] - ETA: 0s
3/3 [==============================] - 0s 5ms/step

2/3 [===================>..........] - ETA: 0s
3/3 [==============================] - 0s 5ms/step
---------------------------- Captured stderr call -----------------------------
Using TensorFlow backend.
WARNING:tensorflow:From C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2020-10-04 14:44:20.180759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-10-04 14:44:20.302134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:73:00.0
2020-10-04 14:44:20.304143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-04 14:44:20.309789: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2020-10-04 14:44:20.313934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2020-10-04 14:44:20.315648: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2020-10-04 14:44:20.320349: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2020-10-04 14:44:20.323542: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2020-10-04 14:44:20.333374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-04 14:44:20.334368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-04 14:44:20.334973: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-10-04 14:44:20.347369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:73:00.0
2020-10-04 14:44:20.348003: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-04 14:44:20.348367: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2020-10-04 14:44:20.348709: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2020-10-04 14:44:20.349046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2020-10-04 14:44:20.349386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2020-10-04 14:44:20.349729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2020-10-04 14:44:20.350077: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-04 14:44:20.350914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-04 14:44:21.298252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-04 14:44:21.298668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-10-04 14:44:21.298904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-10-04 14:44:21.299641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8686 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:73:00.0, compute capability: 7.5)
WARNING:tensorflow:From C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

2020-10-04 14:44:21.888006: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
------------------------------ Captured log call ------------------------------
WARNING  tensorflow:deprecation.py:506 From C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING  tensorflow:module_wrapper.py:139 From C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.
_____________________________ test_fit_generator ______________________________

    @flaky(rerun_filter=lambda err, *args: issubclass(err[0], AssertionError))
    def test_fit_generator():
        model = get_model(num_outputs=2)
        optimizer = 'rmsprop'
        loss = 'mse'
        loss_weights = [1., 0.5]
    
        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,
                      sample_weight_mode=None)
        tracker_cb = TrackerCallback()
        val_seq = RandomSequence(4)
        out = model.fit_generator(generator=RandomSequence(3),
                                  steps_per_epoch=3,
                                  epochs=5,
                                  initial_epoch=0,
                                  validation_data=val_seq,
                                  validation_steps=3,
                                  max_queue_size=1,
                                  callbacks=[tracker_cb])
        assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
        assert tracker_cb.trained_batches == list(range(3)) * 5
        assert len(val_seq.logs) <= 4 * 5
    
        tracker_cb = TrackerCallback()
        val_seq = RandomSequence(4)
        out = model.fit(RandomSequence(3),
                        steps_per_epoch=3,
                        epochs=5,
                        initial_epoch=0,
                        validation_data=val_seq,
                        validation_steps=3,
                        max_queue_size=1,
                        callbacks=[tracker_cb])
        assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
        assert tracker_cb.trained_batches == list(range(3)) * 5
        assert len(val_seq.logs) <= 4 * 5
    
        # steps_per_epoch will be equal to len of sequence if it's unspecified
        tracker_cb = TrackerCallback()
        val_seq = RandomSequence(4)
        out = model.fit_generator(generator=RandomSequence(3),
                                  epochs=5,
                                  initial_epoch=0,
                                  validation_data=val_seq,
                                  callbacks=[tracker_cb],
                                  max_queue_size=1)
        assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]
        assert tracker_cb.trained_batches == list(range(12)) * 5
>       assert 12 * 5 <= len(val_seq.logs) <= (12 * 5) + 2  # the queue may be full.
E       assert (12 * 5) <= 57
E        +  where 57 = len([0, 1, 2, 3, 4, 5, ...])
E        +    where [0, 1, 2, 3, 4, 5, ...] = <test_training.RandomSequence object at 0x00000227B4C3BDD8>.logs

test_training.py:520: AssertionError
---------------------------- Captured stdout call -----------------------------
Epoch 1/5

1/3 [=========>....................] - ETA: 0s - loss: 0.9698 - dense_1_loss: 0.7820 - dropout_loss: 0.3755
3/3 [==============================] - 0s 47ms/step - loss: 1.0458 - dense_1_loss: 0.8256 - dropout_loss: 0.4405 - val_loss: 0.6787 - val_dense_1_loss: 0.7645 - val_dropout_loss: 0.1564
Epoch 2/5

1/3 [=========>....................] - ETA: 0s - loss: 1.2471 - dense_1_loss: 0.9001 - dropout_loss: 0.6940
3/3 [==============================] - 0s 10ms/step - loss: 1.2259 - dense_1_loss: 0.9660 - dropout_loss: 0.5198 - val_loss: 0.8311 - val_dense_1_loss: 0.6318 - val_dropout_loss: 0.2090
Epoch 3/5

1/3 [=========>....................] - ETA: 0s - loss: 1.2081 - dense_1_loss: 0.9553 - dropout_loss: 0.5056
3/3 [==============================] - 0s 10ms/step - loss: 1.2010 - dense_1_loss: 0.9213 - dropout_loss: 0.5595 - val_loss: 0.9140 - val_dense_1_loss: 0.8590 - val_dropout_loss: 0.1879
Epoch 4/5

1/3 [=========>....................] - ETA: 0s - loss: 0.8729 - dense_1_loss: 0.6295 - dropout_loss: 0.4868
3/3 [==============================] - 0s 10ms/step - loss: 1.0731 - dense_1_loss: 0.7524 - dropout_loss: 0.6413 - val_loss: 0.5022 - val_dense_1_loss: 0.6074 - val_dropout_loss: 0.1660
Epoch 5/5

1/3 [=========>....................] - ETA: 0s - loss: 0.9950 - dense_1_loss: 0.7492 - dropout_loss: 0.4915
3/3 [==============================] - 0s 36ms/step - loss: 0.8628 - dense_1_loss: 0.6658 - dropout_loss: 0.3940 - val_loss: 0.6988 - val_dense_1_loss: 0.6850 - val_dropout_loss: 0.1242
Epoch 1/5

1/3 [=========>....................] - ETA: 0s - loss: 1.4118 - dense_1_loss: 1.2399 - dropout_loss: 0.3438
3/3 [==============================] - 0s 16ms/step - loss: 1.2694 - dense_1_loss: 0.9604 - dropout_loss: 0.6181 - val_loss: 0.7638 - val_dense_1_loss: 0.6552 - val_dropout_loss: 0.2233
Epoch 2/5

1/3 [=========>....................] - ETA: 0s - loss: 1.0497 - dense_1_loss: 0.8994 - dropout_loss: 0.3007
3/3 [==============================] - 0s 10ms/step - loss: 1.0016 - dense_1_loss: 0.8245 - dropout_loss: 0.3542 - val_loss: 0.5138 - val_dense_1_loss: 0.6695 - val_dropout_loss: 0.2277
Epoch 3/5

1/3 [=========>....................] - ETA: 0s - loss: 0.9207 - dense_1_loss: 0.7606 - dropout_loss: 0.3202
3/3 [==============================] - 0s 10ms/step - loss: 1.0931 - dense_1_loss: 0.9193 - dropout_loss: 0.3478 - val_loss: 0.6672 - val_dense_1_loss: 0.6739 - val_dropout_loss: 0.1827
Epoch 4/5

1/3 [=========>....................] - ETA: 0s - loss: 0.9995 - dense_1_loss: 0.6482 - dropout_loss: 0.7026
3/3 [==============================] - 0s 10ms/step - loss: 0.8616 - dense_1_loss: 0.5956 - dropout_loss: 0.5319 - val_loss: 0.7181 - val_dense_1_loss: 0.7487 - val_dropout_loss: 0.2121
Epoch 5/5

1/3 [=========>....................] - ETA: 0s - loss: 0.9134 - dense_1_loss: 0.6554 - dropout_loss: 0.5159
3/3 [==============================] - 0s 42ms/step - loss: 1.1095 - dense_1_loss: 0.8605 - dropout_loss: 0.4979 - val_loss: 0.7181 - val_dense_1_loss: 0.7942 - val_dropout_loss: 0.1875
Epoch 1/5

 1/12 [=>............................] - ETA: 0s - loss: 0.6863 - dense_1_loss: 0.5168 - dropout_loss: 0.3391
10/12 [========================>.....] - ETA: 0s - loss: 0.8958 - dense_1_loss: 0.6248 - dropout_loss: 0.5419
12/12 [==============================] - 0s 12ms/step - loss: 0.8688 - dense_1_loss: 0.6067 - dropout_loss: 0.5242 - val_loss: 0.6458 - val_dense_1_loss: 0.6273 - val_dropout_loss: 0.1784
Epoch 2/5

 1/12 [=>............................] - ETA: 0s - loss: 0.6118 - dense_1_loss: 0.4433 - dropout_loss: 0.3369
 9/12 [=====================>........] - ETA: 0s - loss: 0.9497 - dense_1_loss: 0.7252 - dropout_loss: 0.4489
12/12 [==============================] - 0s 18ms/step - loss: 0.8768 - dense_1_loss: 0.6644 - dropout_loss: 0.4249 - val_loss: 0.5763 - val_dense_1_loss: 0.6111 - val_dropout_loss: 0.1704
Epoch 3/5

 1/12 [=>............................] - ETA: 0s - loss: 1.0811 - dense_1_loss: 0.7095 - dropout_loss: 0.7431
11/12 [==========================>...] - ETA: 0s - loss: 0.7925 - dense_1_loss: 0.5522 - dropout_loss: 0.4806
12/12 [==============================] - 0s 20ms/step - loss: 0.7866 - dense_1_loss: 0.5475 - dropout_loss: 0.4783 - val_loss: 0.6021 - val_dense_1_loss: 0.6263 - val_dropout_loss: 0.1652
Epoch 4/5

 1/12 [=>............................] - ETA: 0s - loss: 0.7909 - dense_1_loss: 0.4591 - dropout_loss: 0.6636
 8/12 [===================>..........] - ETA: 0s - loss: 0.6684 - dense_1_loss: 0.4456 - dropout_loss: 0.4455
12/12 [==============================] - 0s 20ms/step - loss: 0.7291 - dense_1_loss: 0.4977 - dropout_loss: 0.4627 - val_loss: 0.4455 - val_dense_1_loss: 0.5453 - val_dropout_loss: 0.1546
Epoch 5/5

 1/12 [=>............................] - ETA: 0s - loss: 0.8486 - dense_1_loss: 0.4890 - dropout_loss: 0.7192
 9/12 [=====================>........] - ETA: 0s - loss: 0.7792 - dense_1_loss: 0.4838 - dropout_loss: 0.5909
12/12 [==============================] - 0s 20ms/step - loss: 0.7595 - dense_1_loss: 0.4730 - dropout_loss: 0.5732 - val_loss: 0.7034 - val_dense_1_loss: 0.4962 - val_dropout_loss: 0.2009
---------------------------- Captured stderr call -----------------------------
2020-10-04 14:44:27.158038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:73:00.0
2020-10-04 14:44:27.158636: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-04 14:44:27.158987: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2020-10-04 14:44:27.159334: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2020-10-04 14:44:27.159673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2020-10-04 14:44:27.160013: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2020-10-04 14:44:27.160358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2020-10-04 14:44:27.160707: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-04 14:44:27.161277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-04 14:44:27.161591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-04 14:44:27.161944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-10-04 14:44:27.162164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-10-04 14:44:27.162661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8686 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:73:00.0, compute capability: 7.5)
---------------------------- Captured stdout call -----------------------------
Epoch 1/5

1/3 [=========>....................] - ETA: 0s - loss: 0.4933 - dense_1_loss: 0.3452 - dropout_loss: 0.2961
3/3 [==============================] - 0s 52ms/step - loss: 0.5053 - dense_1_loss: 0.3580 - dropout_loss: 0.2946 - val_loss: 0.4773 - val_dense_1_loss: 0.4184 - val_dropout_loss: 0.0965
Epoch 2/5

1/3 [=========>....................] - ETA: 0s - loss: 0.7584 - dense_1_loss: 0.2868 - dropout_loss: 0.9432
3/3 [==============================] - 0s 10ms/step - loss: 0.7159 - dense_1_loss: 0.3748 - dropout_loss: 0.6822 - val_loss: 0.3504 - val_dense_1_loss: 0.2545 - val_dropout_loss: 0.1545
Epoch 3/5

1/3 [=========>....................] - ETA: 0s - loss: 0.5400 - dense_1_loss: 0.2806 - dropout_loss: 0.5189
3/3 [==============================] - 0s 10ms/step - loss: 0.5638 - dense_1_loss: 0.3225 - dropout_loss: 0.4825 - val_loss: 0.3012 - val_dense_1_loss: 0.2541 - val_dropout_loss: 0.1027
Epoch 4/5

1/3 [=========>....................] - ETA: 0s - loss: 0.6864 - dense_1_loss: 0.4161 - dropout_loss: 0.5405
3/3 [==============================] - 0s 10ms/step - loss: 0.5725 - dense_1_loss: 0.3254 - dropout_loss: 0.4941 - val_loss: 0.2413 - val_dense_1_loss: 0.2259 - val_dropout_loss: 0.1633
Epoch 5/5

1/3 [=========>....................] - ETA: 0s - loss: 0.4805 - dense_1_loss: 0.2625 - dropout_loss: 0.4360
3/3 [==============================] - 0s 36ms/step - loss: 0.3899 - dense_1_loss: 0.2277 - dropout_loss: 0.3244 - val_loss: 0.4332 - val_dense_1_loss: 0.2661 - val_dropout_loss: 0.1976
Epoch 1/5

1/3 [=========>....................] - ETA: 0s - loss: 0.5314 - dense_1_loss: 0.3092 - dropout_loss: 0.4444
3/3 [==============================] - 0s 16ms/step - loss: 0.5444 - dense_1_loss: 0.3039 - dropout_loss: 0.4810 - val_loss: 0.2377 - val_dense_1_loss: 0.3001 - val_dropout_loss: 0.0950
Epoch 2/5

1/3 [=========>....................] - ETA: 0s - loss: 0.4344 - dense_1_loss: 0.2781 - dropout_loss: 0.3125
3/3 [==============================] - 0s 10ms/step - loss: 0.5239 - dense_1_loss: 0.2512 - dropout_loss: 0.5453 - val_loss: 0.4638 - val_dense_1_loss: 0.3171 - val_dropout_loss: 0.2845
Epoch 3/5

1/3 [=========>....................] - ETA: 0s - loss: 0.7133 - dense_1_loss: 0.2354 - dropout_loss: 0.9557
3/3 [==============================] - 0s 10ms/step - loss: 0.6293 - dense_1_loss: 0.2342 - dropout_loss: 0.7902 - val_loss: 0.4728 - val_dense_1_loss: 0.3527 - val_dropout_loss: 0.1545
Epoch 4/5

1/3 [=========>....................] - ETA: 0s - loss: 0.5012 - dense_1_loss: 0.1742 - dropout_loss: 0.6542
3/3 [==============================] - 0s 10ms/step - loss: 0.5287 - dense_1_loss: 0.2621 - dropout_loss: 0.5333 - val_loss: 0.4056 - val_dense_1_loss: 0.2680 - val_dropout_loss: 0.2265
Epoch 5/5

1/3 [=========>....................] - ETA: 0s - loss: 0.4841 - dense_1_loss: 0.2275 - dropout_loss: 0.5131
3/3 [==============================] - 0s 42ms/step - loss: 0.4888 - dense_1_loss: 0.2371 - dropout_loss: 0.5035 - val_loss: 0.3859 - val_dense_1_loss: 0.2569 - val_dropout_loss: 0.1833
Epoch 1/5

 1/12 [=>............................] - ETA: 0s - loss: 0.3870 - dense_1_loss: 0.2263 - dropout_loss: 0.3214
10/12 [========================>.....] - ETA: 0s - loss: 0.5185 - dense_1_loss: 0.2913 - dropout_loss: 0.4545
12/12 [==============================] - 0s 12ms/step - loss: 0.5181 - dense_1_loss: 0.3018 - dropout_loss: 0.4325 - val_loss: 0.4632 - val_dense_1_loss: 0.3010 - val_dropout_loss: 0.1775
Epoch 2/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4619 - dense_1_loss: 0.1454 - dropout_loss: 0.6329
 9/12 [=====================>........] - ETA: 0s - loss: 0.5051 - dense_1_loss: 0.2470 - dropout_loss: 0.5161
12/12 [==============================] - 0s 18ms/step - loss: 0.5069 - dense_1_loss: 0.2424 - dropout_loss: 0.5289 - val_loss: 0.2557 - val_dense_1_loss: 0.2143 - val_dropout_loss: 0.1465
Epoch 3/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4993 - dense_1_loss: 0.1858 - dropout_loss: 0.6272
10/12 [========================>.....] - ETA: 0s - loss: 0.5217 - dense_1_loss: 0.2267 - dropout_loss: 0.5900
12/12 [==============================] - 0s 20ms/step - loss: 0.5218 - dense_1_loss: 0.2351 - dropout_loss: 0.5734 - val_loss: 0.2747 - val_dense_1_loss: 0.2172 - val_dropout_loss: 0.1557
Epoch 4/5

 1/12 [=>............................] - ETA: 0s - loss: 0.2781 - dense_1_loss: 0.0887 - dropout_loss: 0.3788
10/12 [========================>.....] - ETA: 0s - loss: 0.4256 - dense_1_loss: 0.1898 - dropout_loss: 0.4715
12/12 [==============================] - 0s 20ms/step - loss: 0.4271 - dense_1_loss: 0.1949 - dropout_loss: 0.4643 - val_loss: 0.2871 - val_dense_1_loss: 0.2465 - val_dropout_loss: 0.2007
Epoch 5/5

 1/12 [=>............................] - ETA: 0s - loss: 0.5162 - dense_1_loss: 0.3011 - dropout_loss: 0.4302
 9/12 [=====================>........] - ETA: 0s - loss: 0.4595 - dense_1_loss: 0.2189 - dropout_loss: 0.4813
12/12 [==============================] - 0s 20ms/step - loss: 0.4551 - dense_1_loss: 0.2207 - dropout_loss: 0.4688 - val_loss: 0.3577 - val_dense_1_loss: 0.2185 - val_dropout_loss: 0.1886
---------------------------- Captured stderr call -----------------------------
2020-10-04 14:44:29.596475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:73:00.0
2020-10-04 14:44:29.597075: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-10-04 14:44:29.597424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2020-10-04 14:44:29.597773: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2020-10-04 14:44:29.598111: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2020-10-04 14:44:29.598450: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2020-10-04 14:44:29.598798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2020-10-04 14:44:29.599149: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-04 14:44:29.599720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-04 14:44:29.600035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-04 14:44:29.600387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-10-04 14:44:29.600606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-10-04 14:44:29.601110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8686 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:73:00.0, compute capability: 7.5)
============================== warnings summary ===============================
C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\_pytest\config\__init__.py:1040
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\_pytest\config\__init__.py:1040: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: flaky
    self._mark_plugins_for_rewrite(hook)

test_training.py::test_model_with_partial_loss
test_training.py::test_model_with_external_loss
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_utils.py:819: UserWarning: Output dense_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_1.
    'be expecting any data to be passed to {0}.'.format(name))

test_training.py::test_model_with_external_loss
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_utils.py:819: UserWarning: Output dropout missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dropout.
    'be expecting any data to be passed to {0}.'.format(name))

test_training.py::test_model_with_external_loss
  C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_utils.py:819: UserWarning: Output dense_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_2.
    'be expecting any data to be passed to {0}.'.format(name))

-- Docs: https://docs.pytest.org/en/stable/warnings.html
===Flaky Test Report===

test_model_methods failed and was not selected for rerun.
	<class 'IndexError'>
	list index out of range
	[<TracebackEntry C:\Users\mutation\Desktop\testcase\tests\keras\engine\test_training.py:367>, <TracebackEntry C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\legacy\interfaces.py:91>, <TracebackEntry C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training.py:1791>, <TracebackEntry C:\ProgramData\Anaconda3\envs\keras\lib\site-packages\keras\engine\training_generator.py:420>]
test_fit_generator failed (1 runs remaining out of 2).
	<class 'AssertionError'>
	assert (12 * 5) <= 57
 +  where 57 = len([0, 1, 2, 3, 4, 5, ...])
 +    where [0, 1, 2, 3, 4, 5, ...] = <test_training.RandomSequence object at 0x00000227B4CDBF28>.logs
	[<TracebackEntry C:\Users\mutation\Desktop\testcase\tests\keras\engine\test_training.py:520>]
test_fit_generator failed; it passed 0 out of the required 1 times.
	<class 'AssertionError'>
	assert (12 * 5) <= 57
 +  where 57 = len([0, 1, 2, 3, 4, 5, ...])
 +    where [0, 1, 2, 3, 4, 5, ...] = <test_training.RandomSequence object at 0x00000227B4C3BDD8>.logs
	[<TracebackEntry C:\Users\mutation\Desktop\testcase\tests\keras\engine\test_training.py:520>]

===End Flaky Test Report===
=========================== short test summary info ===========================
FAILED test_training.py::test_model_methods - IndexError: list index out of r...
FAILED test_training.py::test_fit_generator - assert (12 * 5) <= 57
============ 2 failed, 31 passed, 1 skipped, 5 warnings in 30.04s =============
